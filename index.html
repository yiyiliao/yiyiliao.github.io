<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Yiyi Liao


</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link
  defer
  rel="stylesheet"
  type="text/css"
  href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&display=swap"
>

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="/assets/css/jekyll-pygments-themes/github.css">

<!-- Styles -->

<!--<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>/assets/img/xd.png</text></svg>">-->
<link rel="icon" href="/assets/img/xd.png">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">




  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="navbar-brand social">
          <a href="mailto:%79%69%79%69.%6C%69%61%6F@%7A%6A%75.%65%64%75.%63%6E"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=lTBMax0AAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/yiyiliao" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>















        </div>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/xdlab/">
                X-D Lab
                
              </a>
          </li>
          
          
          
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     Yiyi Liao
    </h1>
     <p class="desc">Zhejiang University</p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        


<img class="img-fluid z-depth-1 rounded" src="" srcset="/assets/img/prof_pic.jpg 384w">

      
      
    </div>
    

    <div class="clearfix">
      <p>I am an assistant professor in <a href="https://en.wikipedia.org/wiki/Zhejiang_University" target="_blank" rel="noopener noreferrer">Zhejiang University</a>, leading the <a href="https://yiyiliao.github.io/xdlab">X-Dimensional Representations Lab (X-D Lab)</a>. Before that, I was a Postdoc in <a href="http://cvlibs.net/" target="_blank" rel="noopener noreferrer">Autonomous Vision Group</a> at the University of Tübingen and the MPI for Intelligent Systems, working with <a href="https://avg.is.tuebingen.mpg.de/person/ageiger" target="_blank" rel="noopener noreferrer">Prof. Andreas Geiger</a>. I received my Ph.D. in Control Science and Engineering from <a href="https://en.wikipedia.org/wiki/Zhejiang_University" target="_blank" rel="noopener noreferrer">Zhejiang University</a> in June 2018, under the supervision of <a href="https://april.zju.edu.cn/team/dr-yong-liu/" target="_blank" rel="noopener noreferrer">Prof. Yong Liu</a>, and my B.S. degree from <a href="https://en.wikipedia.org/wiki/Xi%27an_Jiaotong_University" target="_blank" rel="noopener noreferrer">Xi’an Jiaotong University</a> in 2013.</p>

<p>My research interest lies in 3D computer vision, including scene understanding, 3D reconstruction and 3D generative models.</p>

<p>For prospective students interested in computer vision, feel free to contact me via <a href="mailto:yiyi.liao@zju.edu.cn">email</a>!</p>


      
    </div>

    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive" style="height: 250px; overflow-y:scroll">
      <table class="table table-sm table-borderless" style="width: 100%">
        <colgroup>
           <col span="1" style="width: 15%;">
           <col span="1" style="width: 85%;">
        </colgroup>
      <!-- Put <thead>, <tbody>, and <tr>'s here! -->
      <tbody>
        
        
          <tr>
            <th scope="row">Dec 17, 2025</th>
            <td>
              
                <a href="https://xdimlab.github.io/HUGSIM/" target="_blank" rel="noopener noreferrer">HUGSIM</a> is accepted to TPAMI!

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Dec 4, 2025</th>
            <td>
              
                <a href="https://github.com/JasonLSC/GSCodec_Studio" target="_blank" rel="noopener noreferrer">GSCodec Studio</a> is accepted to TCSVT.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Sep 22, 2025</th>
            <td>
              
                I am honored to receive the IEEE <a href="https://attend.ieee.org/mmsp-2025/awards/" target="_blank" rel="noopener noreferrer">MMSP Rising Star Award</a>!

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Sep 18, 2025</th>
            <td>
              
                <a href="https://xdimlab.github.io/Orientation_Matters/" target="_blank" rel="noopener noreferrer">Orientation Matters</a> is accepted to NeurIPS 2025.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Aug 3, 2025</th>
            <td>
              
                <a href="https://freemty.github.io/project-urbangen/" target="_blank" rel="noopener noreferrer">UrbanGen</a> is accepted to TPAMI!

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Jun 30, 2025</th>
            <td>
              
                We are organizing the <a href="https://realadsim.github.io/2025/" target="_blank" rel="noopener noreferrer">RealADSim</a> Workshop &amp; Challenges @ ICCV 2025!

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Jun 25, 2025</th>
            <td>
              
                <a href="https://xdimlab.github.io/Vivid4D/" target="_blank" rel="noopener noreferrer">Vivid-4D</a> is accepted to ICCV.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Jun 18, 2025</th>
            <td>
              
                <a href="https://fuxiao0719.github.io/projects/panopticnerf360/" target="_blank" rel="noopener noreferrer">PanopticNeRF-360</a> is accepted to TPAMI!

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Feb 27, 2025</th>
            <td>
              
                Five papers are accepted to CVPR 2025.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Jul 1, 2024</th>
            <td>
              
                <a href="https://xdimlab.github.io/TeFF/" target="_blank" rel="noopener noreferrer">TEFF</a>, <a href="https://xdimlab.github.io/EDUS/" target="_blank" rel="noopener noreferrer">EDUS</a>, and <a href="https://xdimlab.github.io/REFRAME/" target="_blank" rel="noopener noreferrer">REFRAME</a> are accepted to ECCV 2024.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">May 16, 2024</th>
            <td>
              
                <a href="https://arxiv.org/pdf/2311.09525.pdf" target="_blank" rel="noopener noreferrer">NGEL-SLAM</a> won the ICRA Best Paper Award in Robot Vision!

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Mar 21, 2024</th>
            <td>
              
                I will serve as a Program Chair for 3DV 2025.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Feb 27, 2024</th>
            <td>
              
                Two papers are accepted to CVPR 2024.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Jul 15, 2023</th>
            <td>
              
                Three papers are accepted to ICCV 2023.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Mar 17, 2023</th>
            <td>
              
                I will serve as an Area Chair for NeurIPS 2023.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Feb 28, 2023</th>
            <td>
              
                Three papers are accepted to CVPR 2023.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Sep 16, 2022</th>
            <td>
              
                Our <a href="https://katjaschwarz.github.io/voxgraf/" target="_blank" rel="noopener noreferrer">VoxGRAF</a> is accepted to NeurIPS 2022.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Sep 14, 2022</th>
            <td>
              
                I will serve as an Area Chair for CVPR 2023.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Sep 11, 2022</th>
            <td>
              
                One paper is accepted to CoRL 2022.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Aug 5, 2022</th>
            <td>
              
                Our <a href="https://fuxiao0719.github.io/projects/panopticnerf/" target="_blank" rel="noopener noreferrer">Panoptic NeRF</a> is accepted to 3DV 2022.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Jul 5, 2022</th>
            <td>
              
                Our work on <a href="https://arxiv.org/abs/2203.13572" target="_blank" rel="noopener noreferrer">category-level object pose estimation</a> leveraging 3D-aware generative models is accepted to ECCV 2022.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Jun 6, 2022</th>
            <td>
              
                I was invited to give a talk at the <a href="https://sites.google.com/view/3d-dlad-v4-iv2022/" target="_blank" rel="noopener noreferrer">3D-DLAD workshop</a> at IV 2022.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Jun 4, 2022</th>
            <td>
              
                Our KITTI-360 is accepted to TPAMI and we released all benchmarks! Check out our <a href="https://autonomousvision.github.io/kitti-360/" target="_blank" rel="noopener noreferrer">blog</a> for more information of the benchmarks.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Apr 15, 2022</th>
            <td>
              
                I will serve as an Area Chair for 3DV 2022.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Sep 29, 2021</th>
            <td>
              
                Two papers (1 <a href="http://www.cvlibs.net/publications/Peng2021NEURIPS.pdf" target="_blank" rel="noopener noreferrer">oral</a>, 1 <a href="http://www.cvlibs.net/publications/Schwarz2021NEURIPS.pdf" target="_blank" rel="noopener noreferrer">poster</a>) are accepted to NeurIPS 2021.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Jun 24, 2021</th>
            <td>
              
                Our work <a href="https://github.com/fabiotosi92/SMD-Nets" target="_blank" rel="noopener noreferrer">SMD-Nets</a> was featured on the <a href="https://www.rsipvision.com/CVPR2021-Wednesday/6/" target="_blank" rel="noopener noreferrer">CVPR Daily</a> and the <a href="https://www.rsipvision.com/ComputerVisionNews-2021July/30/" target="_blank" rel="noopener noreferrer">BEST OF CVPR of Computer Vision News</a>.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Jun 2, 2021</th>
            <td>
              
                I will be joining <a href="https://en.wikipedia.org/wiki/Zhejiang_University" target="_blank" rel="noopener noreferrer">Zhejiang University</a> as a tenure-track assistant professor this September!

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">May 31, 2021</th>
            <td>
              
                I will serve as an Area Chair for BMVC 2021.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">May 20, 2021</th>
            <td>
              
                I was acknowledged as <a href="http://cvpr2021.thecvf.com/node/184" target="_blank" rel="noopener noreferrer">Outstanding Reviewer</a> at CVPR 2021.

              
            </td>
          </tr>
        
          <tr>
            <th scope="row">Dec 25, 2020</th>
            <td>
              
                I was invited to give a talk at Graphics And Mixed Environment Seminar (GAMES). Check the <a href="https://yiyiliao.github.io/20201224_GAMES/">interactive slides</a> on 3D controllable image synthesis.

              
            </td>
          </tr>
        
      </tbody>
      <!--
      <tr>
        <td class="date"></td>
      <td class="announcement">
        <a class="all-news" href="https://yiyiliao.github.io/news/">All older news items</a>
      </td>
      </tr>
      -->
      </table>
    </div>
  
</div>

    

    <div class="publications">
    <h2>selected publications</h2>
    <p> Full publication list can be found on <a href="https://scholar.google.com/citations?user=lTBMax0AAAAJ&amp;hl" target="_blank" rel="noopener noreferrer">Google Scholar</a>. <br>
    <sup>*</sup>equal contribution; <sup>♯</sup>corresponding author. </p>
    
    
      <h2 class="year">2025</h2>
      <ol class="bibliography">
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Zhou2025TPAMI.jpg" alt="HUGSIM: A Real-Time, Photo-Realistic and Closed-Loop Simulator for Autonomous Driving" style="width: 100%;">
    </div>
    
  </div>

  <div id="Zhou2025TPAMI" class="col-md-9">
    
      
        <div class="title">HUGSIM: A Real-Time, Photo-Realistic and Closed-Loop Simulator for Autonomous Driving</div>
      
      <div class="author">
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://hyzhou404.github.io/" target="_blank" rel="noopener noreferrer">Zhou, Hongyu</a>,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                  Lin, Longzhong,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
              
            
          
          
          
            
              
                
                  Wang, Jiabao,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://scholar.google.com/citations?user=BUrQL24AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Lu, Yichong</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Dongfeng, Bai,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://scholar.google.com.sg/citations?user=-rCulKwAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Liu, Bingbing</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi<sup>♯</sup></em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI)</em>
      
      
      
        2025
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2412.01718" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/hyzhou404/HUGSIM" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://xdimlab.github.io/HUGSIM/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In the past few decades, autonomous driving algorithms have made significant progress in perception, planning, and control. However, evaluating individual components does not fully reflect the performance of entire systems, highlighting the need for more holistic assessment methods. This motivates the development of HUGSIM, a closed-loop, photo-realistic, and real-time simulator for evaluating autonomous driving algorithms. We achieve this by lifting captured 2D RGB images into the 3D space via 3D Gaussian Splatting, improving the rendering quality for closed-loop scenarios, and building the closed-loop environment. In terms of rendering, We tackle challenges of novel view synthesis in closed-loop scenarios, including viewpoint extrapolation and 360-degree vehicle rendering. Beyond novel view synthesis, HUGSIM further enables the full closed simulation loop, dynamically updating the ego and actor states and observations based on control commands. Moreover, HUGSIM offers a comprehensive benchmark across more than 70 sequences from KITTI-360, Waymo, nuScenes, and PandaSet, along with over 400 varying scenarios, providing a fair and realistic evaluation platform for existing autonomous driving algorithms. HUGSIM not only serves as an intuitive evaluation benchmark but also unlocks the potential for fine-tuning autonomous driving algorithms in a photorealistic closed-loop setting.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Li2025TCSVT.png" alt="GSCodec Studio: A Modular Framework for Gaussian Splat Compression" style="width: 100%;">
    </div>
    
  </div>

  <div id="Li2025TCSVT" class="col-md-9">
    
      
        <div class="title">GSCodec Studio: A Modular Framework for Gaussian Splat Compression</div>
      
      <div class="author">
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://github.com/JasonLSC" target="_blank" rel="noopener noreferrer">Li, Sicheng</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Wu, Chengzhen,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
          
          
          
            
              
                
                  Li, Hao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Gao, Xiang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi<sup>♯</sup></em>,
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://person.zju.edu.cn/en/yul" target="_blank" rel="noopener noreferrer">Yu, Lu<sup>ٯ</sup></a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Trans. on Circuits and Systems for Video Technology (TCSVT)</em>
      
      
      
        2025
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2506.01822" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/JasonLSC/GSCodec_Studio" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>3D Gaussian Splatting and its extension to 4D dynamic scenes enable photorealistic, real-time rendering from real-world captures, positioning Gaussian Splats (GS) as a promising format for next-generation immersive media. However, their high storage requirements pose significant challenges for practical use in sharing, transmission, and storage. Despite various studies exploring GS compression from different perspectives, these efforts remain scattered across separate repositories, complicating benchmarking and the integration of best practices. To address this gap, we present GSCodec Studio, a unified and modular framework for GS reconstruction, compression, and rendering. The framework incorporates a diverse set of 3D/4D GS reconstruction methods and GS compression techniques as modular components, facilitating flexible combinations and comprehensive comparisons. By integrating best practices from community research and our own explorations, GSCodec Studio supports the development of compact representation and compression solutions for static and dynamic Gaussian Splats, namely our Static and Dynamic GSCodec, achieving competitive rate-distortion performance in static and dynamic GS compression. The code for our framework is publicly available online, to advance the research on Gaussian Splats compression.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Xu2025CVMJ.jpg" alt="Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation" style="width: 100%;">
    </div>
    
  </div>

  <div id="Xu2025CVMJ" class="col-md-9">
    
      
        <div class="title">Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation</div>
      
      <div class="author">
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://zhenx.me/" target="_blank" rel="noopener noreferrer">Xu, Zhen<sup>*</sup></a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://hyzhou404.github.io/" target="_blank" rel="noopener noreferrer">Zhou, Hongyu<sup>*</sup></a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://pengsida.net/" target="_blank" rel="noopener noreferrer">Peng, Sida</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://haotongl.github.io/" target="_blank" rel="noopener noreferrer">Lin, Haotong</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
          
          
          
            
              
                
                  Guo, Haoyu,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://jhaoshao.github.io/" target="_blank" rel="noopener noreferrer">Shao, Jiahao</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yang, Peishan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yang, Qinglin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Miao, Sheng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  He, Xingyi,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
              
            
          
          
          
            
              
                
                  Wang, Yifan,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://csse.szu.edu.cn/staff/ruizhenhu/" target="_blank" rel="noopener noreferrer">Hu, Ruizhen</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://xzhou.me/" target="_blank" rel="noopener noreferrer">Zhou, Xiaowei</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cad.zju.edu.cn/home/bao/" target="_blank" rel="noopener noreferrer">Bao, Hujun</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Computational Visual Media Journal</em>
      
      
      
        2025
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2507.11540" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Depth estimation is a fundamental task in 3D computer vision, crucial for applications such as 3D reconstruction, free-viewpoint rendering, robotics, autonomous driving, and AR/VR technologies. Traditional methods relying on hardware sensors like LiDAR are often limited by high costs, low resolution, and environmental sensitivity, limiting their applicability in real-world scenarios. Recent advances in vision-based methods offer a promising alternative, yet they face challenges in generalization and stability due to either the low-capacity model architectures or the reliance on domain-specific and small-scale datasets. The emergence of scaling laws and foundation models in other domains has inspired the development of "depth foundation models": deep neural networks trained on large datasets with strong zero-shot generalization capabilities. This paper surveys the evolution of deep learning architectures and paradigms for depth estimation across the monocular, stereo, multi-view, and monocular video settings. We explore the potential of these models to address existing challenges and provide a comprehensive overview of large-scale datasets that can facilitate their development. By identifying key architectures and training strategies, we aim to highlight the path towards robust depth foundation models, offering insights into their future research and applications.  </p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Lu2025NEURIPS.jpg" alt="Orientation Matters: Making 3D Generative Models Orientation-Aligned" style="width: 100%;">
    </div>
    
  </div>

  <div id="Lu2025NEURIPS" class="col-md-9">
    
      
        <div class="title">Orientation Matters: Making 3D Generative Models Orientation-Aligned</div>
      
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://scholar.google.com/citations?user=BUrQL24AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Lu, Yichong</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Tian, Yuzhuo,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                  Jiang, Zijin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhao, Yikun,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yang, Yuanbo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Ouyang, Hao,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                  Hu, Haoji,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
              
            
          
          
          
            
              
                
                  Yu, Huimin,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://shenyujun.github.io/" target="_blank" rel="noopener noreferrer">Shen, Yujun</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi<sup>♯</sup></em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>
      
      
      
        2025
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2506.08640" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
      <a href="https://xdimlab.github.io/Orientation_Matters/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Humans intuitively perceive object shape and orientation from a single image, guided by strong priors about canonical poses. However, existing 3D generative models often produce misaligned results due to inconsistent training data, limiting their usability in downstream tasks. To address this gap, we introduce the task of orientation-aligned 3D object generation: producing 3D objects from single images with consistent orientations across categories. To facilitate this, we construct Objaverse-OA, a dataset of 14,832 orientation-aligned 3D models spanning 1,008 categories. Leveraging Objaverse-OA, we fine-tune two representative 3D generative models based on multi-view diffusion and 3D variational autoencoder frameworks to produce aligned objects that generalize well to unseen objects across various categories. Experimental results demonstrate the superiority of our method over post-hoc alignment approaches. Furthermore, we showcase downstream applications enabled by our aligned object generation, including zero-shot model-free object orientation estimation via analysis-by-synthesis and efficient arrow-based object manipulation.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Yang2025TPAMI.gif" alt="UrbanGen: Urban Generation with Compositional and Controllable Neural Fields" style="width: 100%;">
    </div>
    
  </div>

  <div id="Yang2025TPAMI" class="col-md-9">
    
      
        <div class="title">UrbanGen: Urban Generation with Compositional and Controllable Neural Fields</div>
      
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Yang, Yuanbo,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://shenyujun.github.io/" target="_blank" rel="noopener noreferrer">Shen, Yujun</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi<sup>♯</sup></em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI)</em>
      
      
      
        2025
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/abstract/document/11130421" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
      <a href="https://freemty.github.io/project-urbangen/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Despite the rapid progress in generative radiance fields, most existing methods focus on object-centric applications and are not able to generate complex urban scenes. In this paper, we propose UrbanGen, a solution for the challenging task of generating urban radiance fields with photorealistic rendering, accurate geometry, high controllability, and diverse city styles. Our key idea is to leverage a coarse 3D panoptic prior, represented by a semantic voxel grid for stuff and bounding boxes for countable objects, to condition a compositional generative radiance field. This panoptic prior simplifies the task of learning complex urban geometry, enables disentanglement of stuff and objects, and provides versatile control over both. Moreover, by combining semantic and geometry losses with adversarial training, our method faithfully adheres to the input conditions, allowing for joint rendering of semantic and depth maps alongside RGB images. In addition, we collect a unified dataset with images and their panoptic priors in the same format from 3 diverse real-world datasets: KITTI-360, nuScenes, and Waymo, and train a city style-aware model on this data. Our systematic study shows that UrbanGen outperforms state-of-the-art generative radiance field baselines in terms of image fidelity and geometry accuracy for urban scene generation. Furthermore, UrbenGen brings a new set of controllability features, including large camera movements, stuff editing, and city style control.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Fu2025TPAMI.gif" alt="PanopticNeRF-360: Panoramic 3D-to-2D Label Transfer in Urban Scenes" style="width: 100%;">
    </div>
    
  </div>

  <div id="Fu2025TPAMI" class="col-md-9">
    
      
        <div class="title">PanopticNeRF-360: Panoramic 3D-to-2D Label Transfer in Urban Scenes</div>
      
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://fuxiao0719.github.io/" target="_blank" rel="noopener noreferrer">Fu, Xiao</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://zhanghe3z.github.io/" target="_blank" rel="noopener noreferrer">Zhang, Shangzhan</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://tianrun-chen.github.io/" target="_blank" rel="noopener noreferrer">Chen, Tianrun</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://scholar.google.com/citations?user=BUrQL24AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Lu, Yichong</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://xzhou.me/" target="_blank" rel="noopener noreferrer">Zhou, Xiaowei</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi<sup>♯</sup></em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI)</em>
      
      
      
        2025
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2309.10815" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/fuxiao0719/PanopticNeRF/tree/panopticnerf360" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://fuxiao0719.github.io/projects/panopticnerf360/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Training perception systems for self-driving cars requires substantial 2D annotations that are labor-intensive to manual label. While existing datasets provide rich annotations on pre-recorded sequences, they fall short in labeling rarely encountered viewpoints, potentially hampering the generalization ability for perception models. In this paper, we present PanopticNeRF-360, a novel approach that combines coarse 3D annotations with noisy 2D semantic cues to generate high-quality panoptic labels and images from any viewpoint. Our key insight lies in exploiting the complementarity of 3D and 2D priors to mutually enhance geometry and semantics. Specifically, we propose to leverage coarse 3D bounding primitives and noisy 2D semantic and instance predictions to guide geometry optimization, by encouraging predicted labels to match panoptic pseudo ground truth. Simultaneously, the improved geometry assists in filtering 3D&amp;2D annotation noise by fusing semantics in 3D space via a learned semantic field. To further enhance appearance, we combine MLP and hash grids to yield hybrid scene features, striking a balance between high-frequency appearance and contiguous semantics. Our experiments demonstrate PanopticNeRF-360’s state-of-the-art performance over label transfer methods on the challenging urban scenes of the KITTI-360 dataset. Moreover, PanopticNeRF-360 enables omnidirectional rendering of high-fidelity, multi-view and spatiotemporally consistent appearance, semantic and instance labels.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Huang2025ICCV.jpg" alt="Vivid4D: Improving 4D Reconstruction from Monocular Video by Video Inpainting" style="width: 100%;">
    </div>
    
  </div>

  <div id="Huang2025ICCV" class="col-md-9">
    
      
        <div class="title">Vivid4D: Improving 4D Reconstruction from Monocular Video by Video Inpainting</div>
      
      <div class="author">
        
          
          
          
          
            
              
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://jaceyhuang.github.io/" target="_blank" rel="noopener noreferrer">Huang, Jiaxin</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Miao, Sheng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yang, Bangbang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Ma, Yuewen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi<sup>♯</sup></em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of the IEEE International Conf. on Computer Vision (ICCV)</em>
      
      
      
        2025
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2504.11092" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
      <a href="https://xdimlab.github.io/Vivid4D/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Reconstructing 4D dynamic scenes from casually captured monocular videos is valuable but highly challenging, as each timestamp is observed from a single viewpoint. We introduce Vivid4D, a novel approach that enhances 4D monocular video synthesis by augmenting observation views — synthesizing multi-view videos from a monocular input. Unlike existing methods that either solely leverage geometric priors for supervision or use generative priors while overlooking geometry, we integrate both. This reformulates view augmentation as a video inpainting task, where observed views are warped into new viewpoints based on monocular depth priors. To achieve this, we train a video inpainting model on unposed web videos with synthetically generated masks that mimic warping occlusions, ensuring spatially and temporally consistent completion of missing regions. To further mitigate inaccuracies in monocular depth priors, we introduce an iterative view augmentation strategy and a robust reconstruction loss. Experiments demonstrate that our method effectively improves monocular 4D scene reconstruction and completion.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Li2025CVPR.jpg" alt="GIFStream: 4D Gaussian-based Immersive Video with Feature Stream" style="width: 100%;">
    </div>
    
  </div>

  <div id="Li2025CVPR" class="col-md-9">
    
      
        <div class="title">GIFStream: 4D Gaussian-based Immersive Video with Feature Stream</div>
      
      <div class="author">
        
          
          
          
          
            
              
            
              
            
          
          
          
            
              
                
                  Li, Hao,
                
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://github.com/JasonLSC" target="_blank" rel="noopener noreferrer">Li, Sicheng</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Gao, Xiang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Batuer, Abudouaihati,
                
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://person.zju.edu.cn/en/yul" target="_blank" rel="noopener noreferrer">Yu, Lu<sup>ٯ</sup></a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi<sup>♯</sup></em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)</em>
      
      
      
        2025
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2505.07539" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
      <a href="https://xdimlab.github.io/GIFStream/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Immersive video offers a 6-Dof-free viewing experience, potentially playing a key role in future video technology. Recently, 4D Gaussian Splatting has gained attention as an effective approach for immersive video due to its high rendering efficiency and quality, though maintaining quality with manageable storage remains challenging. To address this, we introduce GIFStream, a novel 4D Gaussian representation using a canonical space and a deformation field enhanced with time-dependent feature streams. These feature streams enable complex motion modeling and allow efficient compression by leveraging their motion-awareness and temporal correspondence. Additionally, we incorporate both temporal and spatial compression networks for end-to-end compression. Experimental results show that GIFStream delivers high-quality immersive video at 30 Mbps, with real-time rendering and fast decoding on an RTX 4090.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Shao2025CVPR.gif" alt="Learning temporally consistent video depth from video diffusion priors" style="width: 100%;">
    </div>
    
  </div>

  <div id="Shao2025CVPR" class="col-md-9">
    
      
        <div class="title">Learning temporally consistent video depth from video diffusion priors</div>
      
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://jhaoshao.github.io/" target="_blank" rel="noopener noreferrer">Shao, Jiahao</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yang, Yuanbo,
                
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://hyzhou404.github.io/" target="_blank" rel="noopener noreferrer">Zhou, Hongyu</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://youmi-zym.github.io/" target="_blank" rel="noopener noreferrer">Zhang, Youmin</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://shenyujun.github.io/" target="_blank" rel="noopener noreferrer">Shen, Yujun</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://scholar.google.com.br/citations?user=ow3r9ogAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Guizilini, Vitor</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://yuewang.xyz/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://mattpoggi.github.io/" target="_blank" rel="noopener noreferrer">Poggi, Matteo</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi<sup>♯</sup></em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)</em>
      
      
      
        2025
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2406.01493" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
      <a href="https://xdimlab.github.io/ChronoDepth/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This work addresses the challenge of streamed video depth estimation, which expects not only per-frame accuracy but, more importantly, cross-frame consistency. We argue that sharing contextual information between frames or clips is pivotal in fostering temporal consistency. Thus, instead of directly developing a depth estimator from scratch, we reformulate this predictive task into a conditional generation problem to provide contextual information within a clip and across clips. Specifically, we propose a consistent context-aware training and inference strategy for arbitrarily long videos to provide cross-clip context. We sample independent noise levels for each frame within a clip during training while using a sliding window strategy and initializing overlapping frames with previously predicted frames without adding noise. Moreover, we design an effective training strategy to provide context within a clip. Extensive experimental results validate our design choices and demonstrate the superiority of our approach, dubbed ChronoDepth.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Lu2025CVPR.jpg" alt="UrbanCAD: Towards Highly Controllable and Photorealistic 3D Vehicles for Urban Scene Simulation" style="width: 100%;">
    </div>
    
  </div>

  <div id="Lu2025CVPR" class="col-md-9">
    
      
        <div class="title">UrbanCAD: Towards Highly Controllable and Photorealistic 3D Vehicles for Urban Scene Simulation</div>
      
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://scholar.google.com/citations?user=BUrQL24AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Lu, Yichong</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Cai, Yichi,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://zhanghe3z.github.io/" target="_blank" rel="noopener noreferrer">Zhang, Shangzhan</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://hyzhou404.github.io/" target="_blank" rel="noopener noreferrer">Zhou, Hongyu</a>,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                  Hu, Haoji,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
              
            
          
          
          
            
              
                
                  Yu, Huimin,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi<sup>♯</sup></em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)</em>
      
      
      
        2025
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2411.19292" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
      <a href="https://xdimlab.github.io/UrbanCAD/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Photorealistic 3D vehicle models with high controllability are essential for autonomous driving simulation and data augmentation. While handcrafted CAD models provide flexible controllability, free CAD libraries often lack the high-quality materials necessary for photorealistic rendering. Conversely, reconstructed 3D models offer high-fidelity rendering but lack controllability. In this work, we introduce UrbanCAD, a framework that pushes the frontier of the photorealism-controllability trade-off by generating highly controllable and photorealistic 3D vehicle digital twins from a single urban image and a collection of free 3D CAD models and handcrafted materials. These digital twins enable realistic 360-degree rendering, vehicle insertion, material transfer, relighting, and component manipulation such as opening doors and rolling down windows, supporting the construction of long-tail scenarios. To achieve this, we propose a novel pipeline that operates in a retrieval-optimization manner, adapting to observational data while preserving flexible controllability and fine-grained handcrafted details. Furthermore, given multi-view background perspective and fisheye images, we approximate environment lighting using fisheye images and reconstruct the background with 3DGS, enabling the photorealistic insertion of optimized CAD models into rendered novel view backgrounds. Experimental results demonstrate that UrbanCAD outperforms baselines based on reconstruction and retrieval in terms of photorealism. Additionally, we show that various perception models maintain their accuracy when evaluated on UrbanCAD with in-distribution configurations but degrade when applied to realistic out-of-distribution data generated by our method. This suggests that UrbanCAD is a significant advancement in creating photorealistic, safety-critical driving scenarios for downstream applications.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Miao2025CVPR.gif" alt="EVolSplat: Efficient Volume-based Gaussian Splatting for Urban View Synthesis" style="width: 100%;">
    </div>
    
  </div>

  <div id="Miao2025CVPR" class="col-md-9">
    
      
        <div class="title">EVolSplat: Efficient Volume-based Gaussian Splatting for Urban View Synthesis</div>
      
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Miao, Sheng,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://jaceyhuang.github.io/" target="_blank" rel="noopener noreferrer">Huang, Jiaxin</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://scholar.google.com/citations?user=aubQLGYAAAAJ&amp;hl=zh-CN" target="_blank" rel="noopener noreferrer">Bai, Dongfeng</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://yanx27.github.io/" target="_blank" rel="noopener noreferrer">Yan, Xu</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://hyzhou404.github.io/" target="_blank" rel="noopener noreferrer">Zhou, Hongyu</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://scholar.google.com.sg/citations?user=-rCulKwAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Liu, Bingbing</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi<sup>♯</sup></em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)</em>
      
      
      
        2025
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2503.20168" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
      <a href="https://xdimlab.github.io/EVolSplat/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Novel view synthesis of urban scenes is essential for autonomous driving-related applications. Existing NeRF and 3DGS-based methods show promising results in achieving photorealistic renderings but require slow, per-scene optimization. We introduce EVolSplat, an efficient 3D Gaussian Splatting model for urban scenes that works in a feed-forward manner. Unlike existing feed-forward, pixel-aligned 3DGS methods, which often suffer from issues like multi-view inconsistencies and duplicated content, our approach predicts 3D Gaussians across multiple frames within a unified volume using a 3D convolutional network. This is achieved by initializing 3D Gaussians with noisy depth predictions, and then refining their geometric properties in 3D space and predicting color based on 2D textures. Our model also handles distant views and the sky with a flexible hemisphere background model. This enables us to perform fast, feed-forward reconstruction while achieving real-time rendering. Experimental evaluations on the KITTI-360 and Waymo datasets show that our method achieves state-of-the-art quality compared to existing feed-forward 3DGS- and NeRF-based methods. </p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Yang2025CVPR.gif" alt="Prometheus: 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D Scene Generation" style="width: 100%;">
    </div>
    
  </div>

  <div id="Yang2025CVPR" class="col-md-9">
    
      
        <div class="title">Prometheus: 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D Scene Generation</div>
      
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Yang, Yuanbo,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://jhaoshao.github.io/" target="_blank" rel="noopener noreferrer">Shao, Jiahao</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
          
          
          
            
              
                
                  Li, Xinyang,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://shenyujun.github.io/" target="_blank" rel="noopener noreferrer">Shen, Yujun</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi<sup>♯</sup></em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)</em>
      
      
      
        2025
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2412.21117" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
      <a href="https://freemty.github.io/project-prometheus/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this work, we introduce Prometheus, a 3D-aware latent diffusion model for text-to-3D generation at both object and scene levels in seconds. We formulate 3D scene generation as multi-view, feed-forward, pixel-aligned 3D Gaussian generation within the latent diffusion paradigm. To ensure generalizability, we build our model upon pre-trained text-to-image generation model with only minimal adjustments, and further train it using a large number of images from both single-view and multi-view datasets. Furthermore, we introduce an RGB-D latent space into 3D Gaussian generation to disentangle appearance and geometry information, enabling efficient feed-forward generation of 3D Gaussians with better fidelity and geometry. Extensive experimental results demonstrate the effectiveness of our method in both feed-forward 3D Gaussian reconstruction and text-to-3D generation.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
</ol>
    
      <h2 class="year">2024</h2>
      <ol class="bibliography">
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Wu2024RAL.jpg" alt="DORec: Decomposed Object Reconstruction and Segmentation Utilizing 2D Self-Supervised Features" style="width: 100%;">
    </div>
    
  </div>

  <div id="Wu2024RAL" class="col-md-9">
    
      
        <div class="title">DORec: Decomposed Object Reconstruction and Segmentation Utilizing 2D Self-Supervised Features</div>
      
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Wu, Jun,
                
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://github.com/JasonLSC" target="_blank" rel="noopener noreferrer">Li, Sicheng</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Ji, Sihui,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yang, Yifei,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                  Xiong, Rong<sup>♯</sup>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi<sup>♯</sup></em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In IEEE Robotics and Automation Letters (RA-L)</em>
      
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2310.11092.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recovering 3D geometry and textures of individual objects is crucial for many robotics applications, such as manipulation, pose estimation, and autonomous driving. However, decomposing a target object from a complex background is challenging. Most existing approaches rely on costly manual labels to acquire object instance perception. Recent advancements in 2D self-supervised learning offer new prospects for identifying objects of interest, yet leveraging such noisy 2D features for clean decomposition remains difficult. In this paper, we propose a Decomposed Object Reconstruction (DORec) network based on neural implicit representations. Our key idea is to use 2D self-supervised features to create two levels of masks for supervision: a binary mask for foreground regions and a K-cluster mask for semantically similar regions. These complementary masks result in robust decomposition. Experimental results on different datasets show DORec’s superiority in segmenting and reconstructing diverse foreground objects from varied backgrounds enabling downstream tasks such as pose estimation.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Chen2024ECCV.gif" alt="TeFF: Learning 3D-Aware GANs from Unposed Images with Template Feature Field" style="width: 100%;">
    </div>
    
  </div>

  <div id="Chen2024ECCV" class="col-md-9">
    
      
        <div class="title">
          <span>TeFF: Learning 3D-Aware GANs from Unposed Images with Template Feature Field</span>
          <span style="color:red;">(Oral)</span>
        </div>
      
      <div class="author">
        
          
          
          
          
            
              
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://xinyachen21.github.io/" target="_blank" rel="noopener noreferrer">Chen, Xinya</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
          
          
          
            
              
                
                  Guo, Hanlei,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Bin, Yanrui,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://zhanghe3z.github.io/" target="_blank" rel="noopener noreferrer">Zhang, Shangzhan</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yang, Yuanbo,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://shenyujun.github.io/" target="_blank" rel="noopener noreferrer">Shen, Yujun</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi<sup>♯</sup></em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of the European Conf. on Computer Vision (ECCV)</em>
      
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2404.05705.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/XinyaChen21/TeFF/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://xdimlab.github.io/TeFF/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Collecting accurate camera poses of training images has been shown to well serve the learning of 3D-aware generative adversarial networks (GANs) yet can be quite expensive in practice. This work targets learning 3D-aware GANs from unposed images, for which we propose to perform on-the-fly pose estimation of training images with a learned template feature field (TeFF). Concretely, in addition to a generative radiance field as in previous approaches, we ask the generator to also learn a field from 2D semantic features while sharing the density from the radiance field. Such a framework allows us to acquire a canonical 3D feature template leveraging the dataset mean discovered by the generative model, and further efficiently estimate the pose parameters on real data. Experimental results on various challenging datasets demonstrate the superiority of our approach over state-of-the-art alternatives from both the qualitative and the quantitative perspectives.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Ji2024ECCV.jpg" alt="REFRAME: REFlective Surface ReAl-Time Rendering for MobilE Devices" style="width: 100%;">
    </div>
    
  </div>

  <div id="Ji2024ECCV" class="col-md-9">
    
      
        <div class="title">REFRAME: REFlective Surface ReAl-Time Rendering for MobilE Devices</div>
      
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Ji, Chaojie,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
          
          
          
            
              
                
                  Li, Yufeng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi<sup>♯</sup></em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of the European Conf. on Computer Vision (ECCV)</em>
      
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2403.16481.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/MARVELOUSJI/REFRAME" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://xdimlab.github.io/REFRAME/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This work tackles the challenging task of achieving real-time novel view synthesis for reflective surfaces across various scenes. Existing real-time rendering methods, especially those based on meshes, often have subpar performance in modeling surfaces with rich view-dependent appearances. Our key idea lies in leveraging meshes for rendering acceleration while incorporating a novel approach to parameterize view-dependent information. We decompose the color into diffuse and specular, and model the specular color in the reflected direction based on a neural environment map. Our experiments demonstrate that our method achieves comparable reconstruction quality for highly reflective surfaces compared to state-of-the-art offline methods, while also efficiently enabling real-time rendering on edge devices such as smartphones.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Miao2024ECCV.gif" alt="EDUS: Efficient Depth-Guided Urban View Synthesis" style="width: 100%;">
    </div>
    
  </div>

  <div id="Miao2024ECCV" class="col-md-9">
    
      
        <div class="title">EDUS: Efficient Depth-Guided Urban View Synthesis</div>
      
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Miao, Sheng<sup>*</sup>,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://jaceyhuang.github.io/" target="_blank" rel="noopener noreferrer">Huang, Jiaxin<sup>*</sup></a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Dongfeng, Bai,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://weichaoqiu.com/" target="_blank" rel="noopener noreferrer">Qiu, Weichao</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://scholar.google.com.sg/citations?user=-rCulKwAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Liu, Bingbing</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi<sup>♯</sup></em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of the European Conf. on Computer Vision (ECCV)</em>
      
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2407.12395.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/Miaosheng1/EDUS" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://xdimlab.github.io/EDUS/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recent advances in implicit scene representation enable high- fidelity street view novel view synthesis. However, existing methods op- timize a neural radiance field for each scene, relying heavily on dense training images and extensive computation resources. To mitigate this shortcoming, we introduce a new method called Efficient Depth-Guided Urban View Synthesis (EDUS) for fast feed-forward inference and effi- cient per-scene fine-tuning. Different from prior generalizable methods that infer geometry based on feature matching, EDUS leverages noisy predicted geometric priors as guidance to enable generalizable urban view synthesis from sparse input images. The geometric priors allow us to ap- ply our generalizable model directly in the 3D space, gaining robustness across various sparsity levels. Through comprehensive experiments on the KITTI-360 and Waymo datasets, we demonstrate promising gener- alization abilities on novel street scenes. Moreover, our results indicate that EDUS achieves state-of-the-art performance in sparse view settings when combined with fast test-time optimization.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Zhou2024CVPR.gif" alt="HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting" style="width: 100%;">
    </div>
    
  </div>

  <div id="Zhou2024CVPR" class="col-md-9">
    
      
        <div class="title">HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting</div>
      
      <div class="author">
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://hyzhou404.github.io/" target="_blank" rel="noopener noreferrer">Zhou, Hongyu</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://jhaoshao.github.io/" target="_blank" rel="noopener noreferrer">Shao, Jiahao</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
          
          
          
            
              
                
                  Xu, Lu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Dongfeng, Bai,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://weichaoqiu.com/" target="_blank" rel="noopener noreferrer">Qiu, Weichao</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://scholar.google.com.sg/citations?user=-rCulKwAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Liu, Bingbing</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi<sup>♯</sup></em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)</em>
      
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2403.12722.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/hyzhou404/HUGS" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://xdimlab.github.io/hugs_website/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Holistic understanding of urban scenes based on RGB images is a challenging yet important problem. It encompasses understanding both the geometry and appearance to enable novel view synthesis, parsing semantic labels, and tracking moving objects. Despite considerable progress, existing approaches often focus on specific aspects of this task and require additional inputs such as LiDAR scans or manually annotated 3D bounding boxes. In this paper, we introduce a novel pipeline that utilizes 3D Gaussian Splatting for holistic urban scene understanding. Our main idea involves the joint optimization of geometry, appearance, semantics, and motion using a combination of static and dynamic 3D Gaussians, where moving object poses are regularized via physical constraints. Our approach offers the ability to render new viewpoints in real-time, yielding 2D and 3D semantic information with high accuracy, and reconstruct dynamic scenes, even in scenarios where 3D bounding box detection are highly noisy. Experimental results on KITTI, KITTI-360, and Virtual KITTI 2 demonstrate the effectiveness of our approach.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Li2024CVPR.jpg" alt="NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation" style="width: 100%;">
    </div>
    
  </div>

  <div id="Li2024CVPR" class="col-md-9">
    
      
        <div class="title">NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation</div>
      
      <div class="author">
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://github.com/JasonLSC" target="_blank" rel="noopener noreferrer">Li, Sicheng</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
          
          
          
            
              
                
                  Li, Hao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi<sup>♯</sup></em>,
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://person.zju.edu.cn/en/yul" target="_blank" rel="noopener noreferrer">Yu, Lu<sup>ٯ</sup></a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)</em>
      
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2404.02185.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/JasonLSC/NeRFCodec_public" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://jasonlsc.github.io/nerfcodec_homepage/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The emergence of Neural Radiance Fields (NeRF) has greatly impacted 3D scene modeling and novel-view synthesis. As a kind of visual media for 3D scene representation, compression with high rate-distortion performance is an eternal target. Motivated by advances in neural compression and neural field representation, we propose NeRFCodec, an end-to-end NeRF compression framework that integrates non-linear transform, quantization, and entropy coding for memory-efficient scene representation. Since training a non-linear transform directly on a large scale of NeRF feature planes is impractical, we discover that pre-trained neural 2D image codec can be utilized for compressing the features when adding content-specific parameters. Specifically, we reuse neural 2D image codec but modify its encoder and decoder heads, while keeping the other parts of the pre-trained decoder frozen. This allows us to train the full pipeline via supervision of rendering loss and entropy loss, yielding the rate-distortion balance by updating the content-specific parameters. At test time, the bitstreams containing latent code, feature decoder head, and other side information are transmitted for communication. Experimental results demonstrate our method outperforms existing NeRF compression methods, enabling high-quality novel view synthesis with a memory budget of 0.5 MB.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Yunus2024CGF.jpg" alt="Recent Trends in 3D Reconstruction of General Non-Rigid Scenes" style="width: 100%;">
    </div>
    
  </div>

  <div id="Yunus2024CGF" class="col-md-9">
    
      
        <div class="title">Recent Trends in 3D Reconstruction of General Non-Rigid Scenes</div>
      
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://scholar.google.com/citations?user=j0GdrVcAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Yunus, Raza</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://janericlenssen.github.io/" target="_blank" rel="noopener noreferrer">Lenssen, Jan Eric</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://m-niemeyer.github.io/" target="_blank" rel="noopener noreferrer">Niemeyer, Michael</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://chrirupp.github.io/" target="_blank" rel="noopener noreferrer">Rupprecht, Christian</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Theobalt, Christian</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html" target="_blank" rel="noopener noreferrer">Pons-Moll, Gerard</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://jbhuang0604.github.io/" target="_blank" rel="noopener noreferrer">Huang, Jia-Bin</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://people.mpi-inf.mpg.de/~golyanik/" target="_blank" rel="noopener noreferrer">Golyanik, Vladislav</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://cvmp.cs.uni-saarland.de/#eddy-ilg" target="_blank" rel="noopener noreferrer">Ilg, Eddy</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In State-of-the-Art Report at EUROGRAPHICS</em>
      
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2403.15064.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Reconstructing models of the real world, including 3D geometry, appearance, and motion of real scenes, is essential for computer graphics and computer vision. It enables the synthesizing of photorealistic novel views, useful for the movie industry and AR/VR applications. It also facilitates the content creation necessary in computer games and AR/VR by avoiding laborious manual design processes. Further, such models are fundamental for intelligent computing systems that need to interpret real-world scenes and actions to act and interact safely with the human world. Notably, the world surrounding us is dynamic, and reconstructing models of dynamic, non-rigidly moving scenes is a severely underconstrained and challenging problem.  This state-of-the-art report (STAR) offers the reader a comprehensive summary of state-of-the-art techniques with monocular and multi-view inputs such as data from RGB and RGB-D sensors, among others, conveying an understanding of different approaches, their potential applications, and promising further research directions. The report covers 3D reconstruction of general non-rigid scenes and further addresses the techniques for scene decomposition, editing and controlling, and generalizable and generative modeling. More specifically, we first review the common and fundamental concepts necessary to understand and navigate the field and then discuss the state-of-the-art techniques by reviewing recent approaches that use traditional and machine-learning-based neural representations, including a discussion on the newly enabled applications. The STAR is con- cluded with a discussion of the remaining limitations and open challenges.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Mao2024ICRA.jpg" alt="NGEL-SLAM: Neural Implicit Representation-based Global Consistent Low-Latency SLAM System" style="width: 100%;">
    </div>
    
  </div>

  <div id="Mao2024ICRA" class="col-md-9">
    
      
        <div class="title">
          <span>NGEL-SLAM: Neural Implicit Representation-based Global Consistent Low-Latency SLAM System</span>
          <span style="color:red;">(Best Robot Vision Paper)</span>
        </div>
      
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Mao, Yunxuan,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
              
            
          
          
          
            
              
                
                  Yu, Xuan,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
              
            
          
          
          
            
              
                
                  Wang, Kai,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cse.zju.edu.cn/english/redir.php?catalog_id=1113878&amp;object_id=1115898" target="_blank" rel="noopener noreferrer">Xiong, Rong</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi<sup>♯</sup></em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of the IEEE International Conf. on Robotics and Automation (ICRA)</em>
      
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2311.09525.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/YunxuanMao/ngel_slam" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Neural implicit representations have emerged as a promising solution for providing dense geometry in Simultaneous Localization and Mapping (SLAM). However, existing methods in this direction fall short in terms of global consistency and low latency. This paper presents NGEL-SLAM to tackle the above challenges. To ensure global consistency, our system leverages a traditional feature-based tracking module that incorporates loop closure. Additionally, we maintain a global consistent map by representing the scene using multiple neural implicit fields, enabling quick adjustment to the loop closure.  Moreover, our system allows for fast convergence through the use of octree-based implicit representations. The combination of rapid response to loop closure and fast convergence makes our system a truly low-latency system that achieves global consistency. Our system enables rendering high-fidelity RGBD images, along with extracting dense and complete surfaces.  Experiments on both synthetic and real-world datasets suggest that our system achieves state-of-the-art tracking and mapping accuracy while maintaining low latency.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
</ol>
    
      <h2 class="year">2023</h2>
      <ol class="bibliography">
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Shi2023Arxiv.jpg" alt="3D Generative Models: A Survey" style="width: 100%;">
    </div>
    
  </div>

  <div id="Shi2023Arxiv" class="col-md-9">
    
      
        <div class="title">3D Generative Models: A Survey</div>
      
      <div class="author">
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://vivianszf.github.io/" target="_blank" rel="noopener noreferrer">Shi, Zifan<sup>*</sup></a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://pengsida.net/" target="_blank" rel="noopener noreferrer">Peng, Sida<sup>*</sup></a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://justimyhxu.github.io/" target="_blank" rel="noopener noreferrer">Xu, Yinghao<sup>*</sup></a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi<sup>♯</sup></em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://shenyujun.github.io/" target="_blank" rel="noopener noreferrer">Shen, Yujun<sup>ٯ</sup></a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In arXiv.org</em>
      
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2210.15663.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Generative models aim to learn the distribution of observed data by generating new instances. With the advent of neural networks, deep generative models, including variational autoencoders (VAEs), generative adversarial networks (GANs), and diffusion models (DMs), have progressed remarkably in synthesizing 2D images. Recently, researchers started to shift focus from 2D to 3D space, considering that 3D data is more closely aligned with our physical world and holds immense practical potential. However, unlike 2D images, which possess an inherent and efficient representation (i.e., a pixel grid), representing 3D data poses significantly greater challenges. Ideally, a robust 3D representation should be capable of accurately modeling complex shapes and appearances while being highly efficient in handling high-resolution data with high processing speeds and low memory requirements. Regrettably, existing 3D representations, such as point clouds, meshes, and neural fields, often fail to satisfy all of these requirements simultaneously.  In this survey, we thoroughly review the ongoing developments of 3D generative models, including methods that employ 2D and 3D supervision. Our analysis centers on generative models, with a particular focus on the representations utilized in this context. We believe our survey will help the community to track the field’s evolution and to spark innovative ideas to propel progress towards solving this challenging task.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Yu2023RAL.jpg" alt="NF-Atlas: Multi-Volume Neural Feature Fields for Large Scale LiDAR Mapping" style="width: 100%;">
    </div>
    
  </div>

  <div id="Yu2023RAL" class="col-md-9">
    
      
        <div class="title">NF-Atlas: Multi-Volume Neural Feature Fields for Large Scale LiDAR Mapping</div>
      
      <div class="author">
        
          
          
          
          
            
              
            
              
            
              
            
          
          
          
            
              
                
                  Yu, Xuan,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
              
            
              
            
          
          
          
            
              
                
                  Liu, Yili,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Mao, Sitong,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
          
          
          
            
              
                
                  Zhou, Shunbo,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cse.zju.edu.cn/english/redir.php?catalog_id=1113878&amp;object_id=1115898" target="_blank" rel="noopener noreferrer">Xiong, Rong</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In IEEE Robotics and Automation Letters (RA-L)</em>
      
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2304.04624.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/yuxuan1206/NF-Atlas" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://yuxuan1206.github.io/NFAtlas/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p> LiDAR Mapping has been a long-standing problem in robotics. Recent progress in neural implicit representation has brought new opportunities to robotic mapping. In this paper, we propose the multi-volume neural feature fields, called NF-Atlas, which bridge the neural feature volumes with pose graph optimization. By regarding the neural feature volume as pose graph nodes and the relative pose between volumes as pose graph edges, the entire neural feature field becomes both locally rigid and globally elastic. Locally, the neural feature volume employs a sparse feature Octree and a small MLP to encode the signed distance function (SDF) of the submap with an option of semantics. Learning the map using this structure allows for end-to-end solving of maximum a posteriori (MAP) based probabilistic mapping. Globally, the map is built volume by volume independently, avoiding catastrophic forgetting when mapping incrementally. Furthermore, when a loop closure occurs, with the elastic pose graph based representation, only updating the origin of neural volumes is required without remapping. Finally, these functionalities of NF-Atlas are validated. Thanks to the sparsity and the optimization based formulation, NF-Atlas shows competitive performance in terms of accuracy, efficiency and memory usage on both simulation and real-world datasets.  </p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Yang2023ICCV.gif" alt="UrbanGIRAFFE: Representing Urban Scenes as Compositional Generative Neural Feature Fields" style="width: 100%;">
    </div>
    
  </div>

  <div id="Yang2023ICCV" class="col-md-9">
    
      
        <div class="title">UrbanGIRAFFE: Representing Urban Scenes as Compositional Generative Neural Feature Fields</div>
      
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Yang, Yuanbo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yang, Yifei,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
          
          
          
            
              
                
                  Guo, Hanlei,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cse.zju.edu.cn/english/redir.php?catalog_id=1113878&amp;object_id=1115898" target="_blank" rel="noopener noreferrer">Xiong, Rong</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi<sup>♯</sup></em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of the IEEE International Conf. on Computer Vision (ICCV)</em>
      
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2303.14167.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/freemty/urbanGIRAFFE" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://lv3d.github.io/urbanGIRAFFE/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Generating photorealistic images with controllable camera pose and scene contents is essential for many applications including AR/VR and simulation. Despite the fact that rapid progress has been made in 3D-aware generative models, most existing methods focus on object-centric images and are not applicable to generating urban scenes for free camera viewpoint control and scene editing. To address this challenging task, we propose UrbanGIRAFFE, which uses a coarse 3D panoptic prior, including the layout distribution of uncountable stuff and countable objects, to guide a 3D-aware generative model. Our model is compositional and controllable as it breaks down the scene into stuff, objects, and sky. Using stuff prior in the form of semantic voxel grids, we build a conditioned stuff generator that effectively incorporates the coarse semantic and geometry information. The object layout prior further allows us to learn an object generator from cluttered scenes. With proper loss functions, our approach facilitates photorealistic 3D-aware image synthesis with diverse controllability, including large camera movement, stuff editing, and object manipulation. We validate the effectiveness of our model on both synthetic and real-world datasets, including the challenging KITTI-360 dataset.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Chen2023ICCV.jpg" alt="VeRi3D: Generative Vertex-based Radiance Fields for 3D Controllable Human Image Synthesis" style="width: 100%;">
    </div>
    
  </div>

  <div id="Chen2023ICCV" class="col-md-9">
    
      
        <div class="title">VeRi3D: Generative Vertex-based Radiance Fields for 3D Controllable Human Image Synthesis</div>
      
      <div class="author">
        
          
          
          
          
            
              
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://xinyachen21.github.io/" target="_blank" rel="noopener noreferrer">Chen, Xinya</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://jaceyhuang.github.io/" target="_blank" rel="noopener noreferrer">Huang, Jiaxin</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Bin, Yanrui,
                
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://person.zju.edu.cn/en/yul" target="_blank" rel="noopener noreferrer">Yu, Lu</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi<sup>♯</sup></em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of the IEEE International Conf. on Computer Vision (ICCV)</em>
      
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2303.14167.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/XinyaChen21/Veri3d" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://xdimlab.github.io/VeRi3d/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Unsupervised learning of 3D-aware generative adversarial networks has lately made much progress. Some recent work demonstrates promising results of learning human generative models using neural articulated radiance fields, yet their generalization ability and controllability lag behind parametric human models, i.e., they do not perform well when generalizing to novel pose/shape and are not part controllable. To solve these problems, we propose VeRi3D, a generative human vertex-based radiance field parameterized by vertices of the parametric human template, SMPL. We map each 3D point to the local coordinate system defined on its neighboring vertices, and use the corresponding vertex feature and local coordinates for mapping it to color and density values. We demonstrate that our simple approach allows for generating photorealistic human images with free control over camera pose, human pose, shape, as well as enabling part-level editing.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Li2023ICCV.jpg" alt="RICO: Regularizing the Unobservable for Indoor Compositional Reconstruction" style="width: 100%;">
    </div>
    
  </div>

  <div id="Li2023ICCV" class="col-md-9">
    
      
        <div class="title">RICO: Regularizing the Unobservable for Indoor Compositional Reconstruction</div>
      
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://kyleleey.github.io/" target="_blank" rel="noopener noreferrer">Li, Zizhang</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://shawlyu.github.io/" target="_blank" rel="noopener noreferrer">Lyu, Xiaoyang</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Ding, Yuanyuan,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://sallymmx.github.io/" target="_blank" rel="noopener noreferrer">Wang, Mengmeng</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi<sup>♯</sup></em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://person.zju.edu.cn/en/yongliu" target="_blank" rel="noopener noreferrer">Liu, Yong<sup>ٯ</sup></a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of the IEEE International Conf. on Computer Vision (ICCV)</em>
      
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2303.08605.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/kyleleey/RICO" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recently, neural implicit surfaces have become popular for multi-view reconstruction. To facilitate practical applications like scene editing and manipulation, some works extend the framework with semantic masks input for the object-compositional reconstruction rather than the holistic perspective. Though achieving plausible disentanglement, the performance drops significantly when processing the indoor scenes where objects are usually partially observed. We propose RICO to address this by regularizing the unobservable regions for indoor compositional reconstruction. Our key idea is to first regularize the smoothness of the occluded background, which then in turn guides the foreground object reconstruction in unobservable regions based on the object-background relationship. Particularly, we regularize the geometry smoothness of occluded background patches. With the improved background surface, the signed distance function and the reversedly rendered depth of objects can be optimized to bound them within the background range. Extensive experiments show our method outperforms other methods on synthetic and real-world indoor scenes and prove the effectiveness of proposed regularizations.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Li2023CVPR.jpg" alt="SteerNeRF: Accelerating NeRF Rendering via Smooth Viewpoint Trajectory" style="width: 100%;">
    </div>
    
  </div>

  <div id="Li2023CVPR" class="col-md-9">
    
      
        <div class="title">SteerNeRF: Accelerating NeRF Rendering via Smooth Viewpoint Trajectory</div>
      
      <div class="author">
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://github.com/JasonLSC" target="_blank" rel="noopener noreferrer">Li, Sicheng</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
          
          
          
            
              
                
                  Li, Hao,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi<sup>♯</sup></em>,
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://person.zju.edu.cn/en/yul" target="_blank" rel="noopener noreferrer">Yu, Lu<sup>ٯ</sup></a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)</em>
      
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2212.08476.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/JasonLSC/SteerNeRF_official" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://jasonlsc.github.io/SteerNeRF/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Neural Radiance Fields (NeRF) have demonstrated superior novel view synthesis performance but are slow at rendering. To speed up the volume rendering process, many acceleration methods have been proposed at the cost of large memory consumption. To push the frontier of the efficiency-memory trade-off, we explore a new perspective to accelerate NeRF rendering, leveraging a key fact that the viewpoint change is usually smooth and continuous in interactive viewpoint control. This allows us to leverage the information of preceding viewpoints to reduce the number of rendered pixels as well as the number of sampled points along the ray of the remaining pixels. In our pipeline, a low-resolution feature map is rendered first by volume rendering, then a lightweight 2D neural renderer is applied to generate the output image at target resolution leveraging the features of preceding and current frames. We show that the proposed method can achieve competitive rendering quality while reducing the rendering time with little memory overhead, enabling 30FPS at 1080P image resolution with a low memory footprint.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Shi2023CVPR.jpeg" alt="Learning 3D-aware Image Synthesis with Unknown Pose Distribution" style="width: 100%;">
    </div>
    
  </div>

  <div id="Shi2023CVPR" class="col-md-9">
    
      
        <div class="title">Learning 3D-aware Image Synthesis with Unknown Pose Distribution</div>
      
      <div class="author">
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://vivianszf.github.io/" target="_blank" rel="noopener noreferrer">Shi, Zifan</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://shenyujun.github.io/" target="_blank" rel="noopener noreferrer">Shen, Yujun</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://justimyhxu.github.io/" target="_blank" rel="noopener noreferrer">Xu, Yinghao</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://pengsida.net/" target="_blank" rel="noopener noreferrer">Peng, Sida</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://scholar.google.com.hk/citations?user=mbpgOmEAAAAJ" target="_blank" rel="noopener noreferrer">Guo, Sheng</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://cqf.io/" target="_blank" rel="noopener noreferrer">Chen, Qifeng</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://sites.google.com/view/dyyeung" target="_blank" rel="noopener noreferrer">Yeung, Dit-Yan</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)</em>
      
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2301.07702.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/VivianSZF/pof3d" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://vivianszf.github.io/pof3d/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Existing methods for 3D-aware image synthesis largely depend on the 3D pose distribution pre-estimated on the training set. An inaccurate estimation may mislead the model into learning faulty geometry. This work proposes PoF3D that frees generative radiance fields from the requirements of 3D pose priors. We first equip the generator with an efficient pose learner, which is able to infer a pose from a latent code, to approximate the underlying true pose distribution automatically. We then assign the discriminator a task to learn pose distribution under the supervision of the generator and to differentiate real and synthesized images with the predicted pose as the condition. The pose-free generator and the pose-aware discriminator are jointly trained in an adversarial manner. Extensive results on a couple of datasets confirm that the performance of our approach, regarding both image quality and geometry quality, is on par with state of the art. To our best knowledge, PoF3D demonstrates the feasibility of learning high-quality 3D-aware image synthesis without using 3D pose priors for the first time.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/Zhang2023CVPR.gif" alt="Painting 3D Nature in 2D: View Synthesis of Natural Scenes from a Single Semantic Mask" style="width: 100%;">
    </div>
    
  </div>

  <div id="Zhang2023CVPR" class="col-md-9">
    
      
        <div class="title">Painting 3D Nature in 2D: View Synthesis of Natural Scenes from a Single Semantic Mask</div>
      
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://zhanghe3z.github.io/" target="_blank" rel="noopener noreferrer">Zhang, Shangzhan</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://pengsida.net/" target="_blank" rel="noopener noreferrer">Peng, Sida</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://tianrun-chen.github.io/" target="_blank" rel="noopener noreferrer">Chen, Tianrun</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://friedrich-m.github.io/" target="_blank" rel="noopener noreferrer">Mou, Linzhan</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://haotongl.github.io/" target="_blank" rel="noopener noreferrer">Lin, Haotong</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://scholar.google.com/citations?user=Jtmq_m0AAAAJ" target="_blank" rel="noopener noreferrer">Yu, Kaicheng</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://xzhou.me/" target="_blank" rel="noopener noreferrer">Zhou, Xiaowei</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)</em>
      
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2302.07224.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/zhanghe3z/PaintingNature/blob/main/README.md" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://zju3dv.github.io/paintingnature/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We introduce a novel approach that takes a single semantic mask as input to synthesize multi-view consistent color images of natural scenes, trained with a collection of single images from the Internet. Prior works on 3D-aware image synthesis either require multi-view supervision or learning category-level prior for specific classes of objects, which can hardly work for natural scenes. Our key idea to solve this challenging problem is to use a semantic field as the intermediate representation, which is easier to reconstruct from an input semantic mask and then translate to a radiance field with the assistance of off-the-shelf semantic image synthesis models. Experiments show that our method outperforms baseline methods and produces photorealistic, multi-view consistent videos of a variety of natural scenes.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
</ol>
    
      <h2 class="year">2022</h2>
      <ol class="bibliography">
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/voxgraf.gif" alt="VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids" style="width: 100%;">
    </div>
    
  </div>

  <div id="Schwarz2022NEURIPS" class="col-md-9">
    
      
        <div class="title">VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids</div>
      
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://katjaschwarz.github.io/" target="_blank" rel="noopener noreferrer">Schwarz, Katja</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://axelsauer.com/" target="_blank" rel="noopener noreferrer">Sauer, Axel</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://m-niemeyer.github.io/" target="_blank" rel="noopener noreferrer">Niemeyer, Michael</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2206.07695.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/autonomousvision/voxgraf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://katjaschwarz.github.io/voxgraf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>State-of-the-art 3D-aware generative models rely on coordinate-based MLPs to parameterize 3D radiance fields. While demonstrating impressive results, querying an MLP for every sample along each ray leads to slow rendering. Therefore, existing approaches often render low-resolution feature maps and process them with an upsampling network to obtain the final image. Albeit efficient, neural rendering often entangles viewpoint and content such that changing the camera pose results in unwanted changes of geometry or appearance. Motivated by recent results in voxel-based novel view synthesis, we investigate the utility of sparse voxel grid representations for fast and 3D-consistent generative modeling in this paper. Our results demonstrate that monolithic MLPs can indeed be replaced by 3D convolutions when combining sparse voxel grids with progressive growing, free space pruning and appropriate regularization. To obtain a compact representation of the scene and allow for scaling to higher voxel resolutions, our model disentangles the foreground object (modeled in 3D) from the background (modeled in 2D). In contrast to existing approaches, our method requires only a single forward pass to generate a full 3D scene. It hence allows for efficient rendering from arbitrary viewpoints while yielding 3D consistent results with high visual fidelity.
</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/3dv2022.gif" alt="Panoptic NeRF: 3D-to-2D Label Transfer for Panoptic Urban Scene Segmentation" style="width: 100%;">
    </div>
    
  </div>

  <div id="Fu2022THREEDV" class="col-md-9">
    
      
        <div class="title">Panoptic NeRF: 3D-to-2D Label Transfer for Panoptic Urban Scene Segmentation</div>
      
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://fuxiao0719.github.io/" target="_blank" rel="noopener noreferrer">Fu, Xiao<sup>*</sup></a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
          
          
          
            
              
                
                  Zhang, Shangzhan<sup>*</sup>,
                
              
            
          
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://tianrun-chen.github.io/" target="_blank" rel="noopener noreferrer">Chen, Tianrun</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://scholar.google.com/citations?user=BUrQL24AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Lu, Yichong</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://lanyunzhu.site/" target="_blank" rel="noopener noreferrer">Zhu, Lanyun</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://xzhou.me/" target="_blank" rel="noopener noreferrer">Zhou, Xiaowei</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of the International Conf. on 3D Vision (3DV)</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://www.cvlibs.net/publications/Fu2022THREEDV.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/fuxiao0719/PanopticNeRF" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="https://www.cvlibs.net/publications/Fu2022THREEDV_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
      
    
    
    
      <a href="https://fuxiao0719.github.io/projects/panopticnerf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    <a href="https://www.youtube.com/watch?v=5QKTeFLciWo" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Large-scale training data with high-quality annotations is critical for training semantic and instance segmentation models. Unfortunately, pixel-wise annotation is labor-intensive and costly, raising the demand for more efficient labeling strategies. In this work, we present a novel 3D-to-2D label transfer method, Panoptic NeRF, which aims for obtaining per-pixel 2D semantic and instance labels from easy-to-obtain coarse 3D bounding primitives. Our method utilizes NeRF as a differentiable tool to unify coarse 3D annotations and 2D semantic cues transferred from existing datasets. We demonstrate that this combination allows for improved geometry guided by semantic information, enabling rendering of accurate semantic maps across multiple views. Furthermore, this fusion process resolves label ambiguity of the coarse 3D annotations and filters noise in the 2D predictions. By inferring in 3D space and rendering to 2D labels, our 2D semantic and instance labels are multi-view consistent by design. Experimental results show that Panoptic NeRF outperforms existing label transfer methods in terms of accuracy and multi-view consistency on challenging urban scenes of the KITTI-360 dataset.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/eccv2022.jpg" alt="A Visual Navigation Perspective for Category-Level Object Pose Estimation" style="width: 100%;">
    </div>
    
  </div>

  <div id="Guo2022ECCV" class="col-md-9">
    
      
        <div class="title">A Visual Navigation Perspective for Category-Level Object Pose Estimation</div>
      
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.linkedin.com/in/jiaxin-guo-043a6b1b8" target="_blank" rel="noopener noreferrer">Guo, Jiaxin</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://billyzhonghk.wixsite.com/billyzhong" target="_blank" rel="noopener noreferrer">Zhong, Fangxun</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cse.zju.edu.cn/english/redir.php?catalog_id=1113878&amp;object_id=1115898" target="_blank" rel="noopener noreferrer">Xiong, Rong</a>,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.mrc-cuhk.com/people/prof-yun-hui-liu" target="_blank" rel="noopener noreferrer">Liu, Yunhui</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Liao, Yiyi</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of the European Conf. on Computer Vision (ECCV)</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/2203.13572.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/wrld/visual_navigation_pose_estimation" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    <a href="https://youtu.be/bIsP7Necw0Q" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper studies category-level object pose estimation based on a single monocular image. Recent advances in pose-aware generative models have paved the way for addressing this challenging task using analysis-by-synthesis. The idea is to sequentially update a set of latent variables, \eg, pose, shape, and appearance, of the generative model until the generated image best agrees with the observation. However, convergence and efficiency are two challenges of this inference procedure. In this paper, we take a deeper look at the inference of analysis-by-synthesis from the perspective of visual navigation, and investigate what is a good navigation policy for this specific task. We evaluate three different strategies, including gradient descent, reinforcement learning and imitation learning, via thorough comparisons in terms of convergence, robustness and efficiency. Moreover, we show that a simple hybrid approach leads to an effective and efficient solution. We further compare these strategies to state-of-the-art methods, and demonstrate superior performance on synthetic and real-world datasets leveraging off-the-shelf pose-aware generative models.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/kitti360.jpg" alt="KITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D" style="width: 100%;">
    </div>
    
  </div>

  <div id="Liao2022TPAMI" class="col-md-9">
    
      
        <div class="title">
          <span>KITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D</span>
          <span style="color:red;">(ESI Highly Cited Paper)</span>
        </div>
      
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cbsr.ia.ac.cn/users/ynyu/" target="_blank" rel="noopener noreferrer">Xie, Jun</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI)</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Liao2022PAMI.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
      <a href="https://autonomousvision.github.io/kitti-360/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a>
    
    
      <a href="http://www.cvlibs.net/datasets/kitti-360/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>For the last few decades, several major subfields of artificial intelligence including computer vision, graphics, and robotics have progressed largely independently from each other. Recently, however, the community has realized that progress towards robust intelligent systems such as self-driving cars requires a concerted effort across the different fields. This motivated us to develop KITTI-360, successor of the popular KITTI dataset. KITTI-360 is a suburban driving dataset which comprises richer input modalities, comprehensive semantic instance annotations and accurate localization to facilitate research at the intersection of vision, graphics and robotics. For efficient annotation, we created a tool to label 3D scenes with bounding primitives and developed a model that transfers this information into the 2D image domain, resulting in over  150k images and 1B 3D points with coherent semantic instance annotations across 2D and 3D. Moreover, we established benchmarks and baselines for several tasks relevant to mobile perception, encompassing problems from computer vision, graphics, and robotics on the same dataset, e.g., semantic scene understanding, novel view synthesis and semantic SLAM. KITTI-360 will enable progress at the intersection of these research areas and thus contribute towards solving one of today’s grand challenges: the development of fully autonomous self-driving systems.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
</ol>
    
      <h2 class="year">2021</h2>
      <ol class="bibliography">
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/sap.gif" alt="Shape As Points: A Differentiable Poisson Solver" style="width: 100%;">
    </div>
    
  </div>

  <div id="Peng2021NEURIPS" class="col-md-9">
    
      
        <div class="title">
          <span>Shape As Points: A Differentiable Poisson Solver</span>
          <span style="color:red;">(Oral)</span>
        </div>
      
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://pengsongyou.github.io/" target="_blank" rel="noopener noreferrer">Peng, Songyou</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.maxjiang.ml/" target="_blank" rel="noopener noreferrer">Jiang, Chiyu<sup>ٯ</sup></a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi<sup>♯</sup></em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://m-niemeyer.github.io/" target="_blank" rel="noopener noreferrer">Niemeyer, Michael</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://people.inf.ethz.ch/pomarc/" target="_blank" rel="noopener noreferrer">Pollefeys, Marc</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Peng2021NEURIPS.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      
      <a href="http://www.cvlibs.net/publications/Peng2021NEURIPS_supplementary.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a>
      
    
    
    
      <a href="https://github.com/autonomousvision/shape_as_points" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In recent years, neural implicit representations gained popularity in 3D reconstruction due to their expressiveness and flexibility. However, the implicit nature of neural implicit representations results in slow inference times and requires careful initialization. In this paper, we revisit the classic yet ubiquitous point cloud representation and introduce a differentiable point-to-mesh layer using a differentiable formulation of Poisson Surface Reconstruction (PSR) which allows for a GPU-accelerated fast solution of the indicator function given an oriented point cloud. The differentiable PSR layer allows us to efficiently and differentiably bridge the explicit 3D point representation with the 3D mesh via the implicit indicator field, enabling end-to-end optimization of surface reconstruction metrics such as Chamfer distance. This duality between points and meshes hence allows us to represent shapes as oriented point clouds, which are explicit, lightweight and expressive. Compared to neural implicit representations, our Shape-As-Points (SAP) model is more interpretable, lightweight, and accelerates inference time by one order of magnitude. Compared to other explicit representations such as points, patches, and meshes, SAP produces topology-agnostic, watertight manifold surfaces. We demonstrate the effectiveness of SAP on the task of surface reconstruction from unoriented point clouds and learning-based reconstruction.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/frequency_bias.gif" alt="On the Frequency Bias of Generative Models" style="width: 100%;">
    </div>
    
  </div>

  <div id="Schwarz2021NEURIPS" class="col-md-9">
    
      
        <div class="title">On the Frequency Bias of Generative Models</div>
      
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://katjaschwarz.github.io/" target="_blank" rel="noopener noreferrer">Schwarz, Katja</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Schwarz2021NEURIPS.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      
      <a href="http://www.cvlibs.net/publications/Schwarz2021NEURIPS_supplementary.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a>
      
    
    
    
      <a href="https://github.com/autonomousvision/frequency_bias" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The key objective of Generative Adversarial Networks (GANs) is to generate new data
  with the same statistics as the provided training data. However, multiple recent works show that
  state-of-the-art architectures yet struggle to achieve this goal. In particular, they report an
  elevated amount of high frequencies in the spectral statistics which makes it straightforward to
  distinguish real and generated images. Explanations for this phenomenon are controversial: While
  most works attribute the artifacts to the generator, other works point to the discriminator. We
  take a sober look at those explanations and provide insights on what makes proposed measures
  against high-frequency artifacts effective. To achieve this, we first independently assess the
  architectures of both the generator and discriminator and investigate if they exhibit a frequency
  bias that makes learning the distribution of high-frequency content particularly problematic.
  Based on these experiments, we make the following four observations: 1) Different upsampling
  operations bias the generator towards different spectral properties. 2) Checkerboard artifacts
  introduced by upsampling cannot explain the spectral discrepancies alone as the generator is able
  to compensate for these artifacts. 3) The discriminator does not struggle with detecting high
  frequencies per se but rather struggles with frequencies of low magnitude. 4) The downsampling
  operations in the discriminator can impair the quality of the training signal it provides. In
  light of these findings, we analyze proposed measures against high-frequency artifacts in
  state-of-the-art GAN training but find that none of the existing approaches can fully resolve
  spectral artifacts yet. Our results suggest that there is great potential in improving the
  discriminator and that this could be key to match the distribution of the training data more
  closely.
</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/iccv2021.jpg" alt="KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs" style="width: 100%;">
    </div>
    
  </div>

  <div id="Reiser2021ICCV" class="col-md-9">
    
      
        <div class="title">KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs</div>
      
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://avg.is.tuebingen.mpg.de/person/creiser" target="_blank" rel="noopener noreferrer">Reiser, Christian</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://pengsongyou.github.io/" target="_blank" rel="noopener noreferrer">Peng, Songyou</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of the IEEE International Conf. on Computer Vision (ICCV)</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Reiser2021ICCV.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      
      <a href="http://www.cvlibs.net/publications/Reiser2021ICCV_supplementary.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a>
      
    
    
      <a href="https://autonomousvision.github.io/kilonerf/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a>
    
    
      <a href="https://creiser.github.io/kilonerf/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that significant speed-ups are possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by two orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/cvpr2021.jpg" alt="SMD-Nets: Stereo Mixture Density Networks" style="width: 100%;">
    </div>
    
  </div>

  <div id="Tosi2021CVPR" class="col-md-9">
    
      
        <div class="title">SMD-Nets: Stereo Mixture Density Networks</div>
      
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://vision.disi.unibo.it/~ftosi/" target="_blank" rel="noopener noreferrer">Tosi, Fabio</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://avg.is.tuebingen.mpg.de/person/cschmitt" target="_blank" rel="noopener noreferrer">Schmitt, Carolin</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Tosi2021CVPR.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      
      <a href="http://www.cvlibs.net/publications/Tosi2021CVPR_supplementary.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a>
      
    
    
      <a href="https://autonomousvision.github.io/smdnets/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a>
    
    
      <a href="https://github.com/fabiotosi92/SMD-Nets" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Despite stereo matching accuracy has greatly improved by deep learning in the last few years, recovering sharp boundaries and high-resolution outputs efficiently remains challenging. In this paper, we propose Stereo Mixture Density Networks (SMD-Nets), a simple yet effective learning framework compatible with a wide class of 2D and 3D architectures which ameliorates both issues. Specifically, we exploit bimodal mixture densities as output representation and show that this allows for sharp and precise disparity estimates near discontinuities while explicitly modeling the aleatoric uncertainty inherent in the observations. Moreover, we formulate disparity estimation as a continuous problem in the image domain, allowing our model to query disparities at arbitrary spatial precision. We carry out comprehensive experiments on a new high-resolution and highly realistic synthetic stereo dataset, consisting of stereo pairs at 8Mpx resolution, as well as on real-world stereo datasets. Our experiments demonstrate increased depth accuracy near object boundaries and prediction of ultra high-resolution disparity maps on standard GPUs. We demonstrate the flexibility of our technique by improving the performance of a variety of stereo backbones.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/tip2021.jpg" alt="Learning Steering Kernels for Guided Depth Completion" style="width: 100%;">
    </div>
    
  </div>

  <div id="Liu2021TIP" class="col-md-9">
    
      
        <div class="title">Learning Steering Kernels for Guided Depth Completion</div>
      
      <div class="author">
        
          
          
          
          
            
              
            
              
                
                
          
          
          
            
              
                
                  <a href="https://april.zju.edu.cn/team/lina-liu/" target="_blank" rel="noopener noreferrer">Liu, Lina</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi<sup>♯</sup></em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://person.zju.edu.cn/en/yongliu" target="_blank" rel="noopener noreferrer">Liu, Yong<sup>ٯ</sup></a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Trans. on  Image Processing (TIP)</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Liu2021TIP.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/LINA-lln/Steering-KernelNet" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper addresses the guided depth completion
        task in which the goal is to predict a dense depth map given a
        guidance RGB image and sparse depth measurements. Recent
        advances on this problem nurture hopes that one day we can
        acquire accurate and dense depth at a very low cost. A major
        challenge of guided depth completion is to effectively make use
        of extremely sparse measurements, e.g., measurements covering
        less than 1% of the image pixels. In this paper, we propose
        a fully differentiable model that avoids convolving on sparse
        tensors by jointly learning depth interpolation and refinement.
        More specifically, we propose a differentiable kernel regression
        layer that interpolates the sparse depth measurements via learned
        kernels. We further refine the interpolated depth map using
        a residual depth refinement layer which leads to improved
        performance compared to learning absolute depth prediction
        using a vanilla network. We provide experimental evidence that
        our differentiable kernel regression layer not only enables end-to-end training from very sparse measurements using standard
        convolutional network architectures, but also leads to better
        depth interpolation results compared to existing heuristically
        motivated methods. We demonstrate that our method outperforms many state-of-the-art guided depth completion techniques
        on both NYUv2 and KITTI. We further show the generalization
        ability of our method with respect to the density and spatial
        statistics of the sparse depth measurements.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
</ol>
    
      <h2 class="year">2020</h2>
      <ol class="bibliography">
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/graf.gif" alt="GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis" style="width: 100%;">
    </div>
    
  </div>

  <div id="Schwarz2020NEURIPS" class="col-md-9">
    
      
        <div class="title">GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis</div>
      
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://katjaschwarz.github.io/" target="_blank" rel="noopener noreferrer">Schwarz, Katja<sup>*</sup></a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi<sup>*</sup></em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://m-niemeyer.github.io/" target="_blank" rel="noopener noreferrer">Niemeyer, Michael</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems (NeurIPS)</em>
      
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Schwarz2020NEURIPS.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      
      <a href="http://www.cvlibs.net/publications/Schwarz2020NEURIPS_supplementary.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a>
      
    
    
      <a href="https://autonomousvision.github.io/graf/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a>
    
    
      <a href="https://github.com/autonomousvision/graf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="http://www.cvlibs.net/publications/Schwarz2020NEURIPS_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
      
    
    
    
    
    <a href="https://www.youtube.com/watch?v=akQf7WaCOHo" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>While 2D generative adversarial networks have enabled high-resolution image synthesis,
  they largely lack an understanding of the 3D world and the image formation process. Thus, they do
  not provide precise control over camera viewpoint or object pose. To address this problem, several
  recent approaches leverage intermediate voxel-based representations in combination with differentiable
  rendering. However, existing methods either produce low image resolution or fall short in disentangling
  camera and scene properties, e.g., the object identity may vary with the viewpoint. In this paper,
  we propose a generative model for radiance fields which have recently proven successful for novel view
  synthesis of a single scene. In contrast to voxel-based representations, radiance fields are not
  confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene
  properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a
  multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training
  our model from unposed 2D images alone. We systematically analyze our approach on several challenging
  synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful
  representation for generative image synthesis, leading to 3D consistent models that render with high
  fidelity.
</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/controllableGAN.gif" alt="Towards Unsupervised Learning of Generative Models for 3D Controllable Image Synthesis" style="width: 100%;">
    </div>
    
  </div>

  <div id="Liao2020CVPR" class="col-md-9">
    
      
        <div class="title">Towards Unsupervised Learning of Generative Models for 3D Controllable Image Synthesis</div>
      
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi<sup>*</sup></em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://katjaschwarz.github.io/" target="_blank" rel="noopener noreferrer">Schwarz, Katja<sup>*</sup></a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://avg.is.tuebingen.mpg.de/person/lmescheder" target="_blank" rel="noopener noreferrer">Mescheder, Lars</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)</em>
      
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Liao2020CVPR.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      
      <a href="http://www.cvlibs.net/publications/Liao2020CVPR_supplementary.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a>
      
    
    
      <a href="https://autonomousvision.github.io/controllable-gan/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a>
    
    
      <a href="https://github.com/autonomousvision/controllable_image_synthesis" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="http://www.cvlibs.net/publications/Liao2020CVPR_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
      
    
    
    
    
    <a href="https://www.youtube.com/watch?v=ygQCgGC0Lm8" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In recent years, Generative Adversarial Networks have achieved impressive results
   in photorealistic image synthesis. This progress nurtures hopes that one day the classical
   rendering pipeline can be replaced by efficient models that are learned directly from images.
   However, current image synthesis models operate in the 2D domain where disentangling 3D properties
   such as camera viewpoint or object pose is challenging. Furthermore, they lack an interpretable
   and controllable representation. Our key hypothesis is that the image generation process should
   be modeled in 3D space as the physical world surrounding us is intrinsically three-dimensional.
   We define the new task of 3D controllable image synthesis and propose an approach for solving it
   by reasoning both in 3D space and in the 2D image domain. We demonstrate that our model is able to
   disentangle latent 3D factors of simple multi-object scenes in an unsupervised fashion from raw
   images. Compared to pure 2D baselines, it allows for synthesizing scenes that are consistent wrt.
   changes in viewpoint or object pose. We further evaluate various 3D representations in terms of
   their usefulness for this challenging task.
   </p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
</ol>
    
      <h2 class="year">2019</h2>
      <ol class="bibliography"><li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/cvpr2019.jpg" alt="Connecting the Dots: Learning Representations for Active Monocular Depth Estimation" style="width: 100%;">
    </div>
    
  </div>

  <div id="Riegler2019CVPR" class="col-md-9">
    
      
        <div class="title">Connecting the Dots: Learning Representations for Active Monocular Depth Estimation</div>
      
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://griegler.github.io/" target="_blank" rel="noopener noreferrer">Riegler, Gernot<sup>*</sup></a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi<sup>*</sup></em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://donnessime.github.io/" target="_blank" rel="noopener noreferrer">Donné, Simon</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://vladlen.info/" target="_blank" rel="noopener noreferrer">Koltun, Vladlen</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)</em>
      
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Riegler2019CVPR.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      
      <a href="http://www.cvlibs.net/publications/Riegler2019CVPR_supplementary.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a>
      
    
    
    
      <a href="https://github.com/autonomousvision/connecting_the_dots" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="http://www.cvlibs.net/publications/Riegler2019CVPR_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose a technique for depth estimation with a monocular structured-light camera, \ie, a calibrated stereo set-up with one camera and one laser projector. Instead of formulating the depth estimation via a correspondence search problem, we show that a simple convolutional architecture is sufficient for high-quality disparity estimates in this setting. As accurate ground-truth is hard to obtain, we train our model in a self-supervised fashion with a combination of photometric and geometric losses. Further, we demonstrate that the projected pattern of the structured light sensor can be reliably separated from the ambient information. This can then be used to improve depth boundaries in a weakly supervised fashion by modeling the joint statistics of image and depth edges. The model trained in this fashion compares favorably to the state-of-the-art on challenging synthetic and real-world datasets. In addition, we contribute a novel simulator, which allows to benchmark active depth prediction algorithms in controlled conditions.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li></ol>
    
      <h2 class="year">2018</h2>
      <ol class="bibliography">
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/cvpr2018.jpg" alt="Deep Marching Cubes: Learning Explicit Surface Representations" style="width: 100%;">
    </div>
    
  </div>

  <div id="liao2018cvpr" class="col-md-9">
    
      
        <div class="title">Deep Marching Cubes: Learning Explicit Surface Representations</div>
      
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://donnessime.github.io/" target="_blank" rel="noopener noreferrer">Donné, Simon</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)</em>
      
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Liao2018CVPR.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="http://www.cvlibs.net/redirect.php?site=deep_marching_cubes" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Existing learning based solutions to 3D surface prediction cannot be trained end-to-end as they operate on intermediate representations (eg, TSDF) from which 3D surface meshes must be extracted in a post-processing step (eg, via the marching cubes algorithm). In this paper, we investigate the problem of end-to-end 3D surface prediction. We first demonstrate that the marching cubes algorithm is not differentiable and propose an alternative differentiable formulation which we insert as a final layer into a 3D convolutional neural network. We further propose a set of loss functions which allow for training our model with sparse point supervision. Our experiments demonstrate that the model allows for predicting sub-voxel accurate 3D shapes of arbitrary topology. Additionally, it learns to complete shapes and to separate an object’s inside from its outside even in the presence of sparse and incomplete ground truth. We investigate the benefits of our approach on the task of inferring shapes from 3D point clouds. Our model is flexible and can be combined with a variety of shape encoder and shape inference techniques.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/gcpr2018.jpg" alt="On the Integration of Optical Flow and Action Recognition" style="width: 100%;">
    </div>
    
  </div>

  <div id="Sevilla:GCPR:2018" class="col-md-9">
    
      
        <div class="title">On the Integration of Optical Flow and Action Recognition</div>
      
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://people.cs.umass.edu/~lsevilla/" target="_blank" rel="noopener noreferrer">Sevilla-Lara, Laura</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ps.is.tuebingen.mpg.de/person/fguney" target="_blank" rel="noopener noreferrer">Guney, Fatma</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://varunjampani.github.io/" target="_blank" rel="noopener noreferrer">Jampani, Varun</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Geiger, Andreas</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://ps.is.tuebingen.mpg.de/person/black" target="_blank" rel="noopener noreferrer">Black, Michael</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In German Conference on Pattern Recognition (GCPR)</em>
      
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Sevilla-Lara2018GCPR.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Most of the top performing action recognition methods use optical flow as a black box input. Here we take a deeper look at the combination of flow and action recognition, and investigate why optical flow is helpful, what makes a flow method good for action recognition, and how we can make it better. In particular, we investigate the impact of different flow algorithms and input transformations to better understand how these affect a state-of-the-art action recognition method. Furthermore, we fine tune two neural-network flow methods end-to-end on the most widely used action recognition dataset (UCF101). Based on these experiments, we make the following five observations: 1) optical flow is useful for action recognition because it is invariant to appearance, 2) optical flow methods are optimized to minimize end-point-error (EPE), but the EPE of current methods is not well correlated with action recognition performance, 3) for the flow methods tested, accuracy at boundaries and at small displacements is most correlated with action recognition performance, 4) training optical flow to minimize classification error instead of minimizing EPE improves recognition performance, and 5) optical flow learned for the task of action recognition differs from traditional optical flow especially inside the human body and at the boundary of the body. These observations may encourage optical flow researchers to look beyond EPE as a goal and guide action recognition researchers to seek better motion cues, leading to a tighter integration of the optical flow and action recognition communities.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
</ol>
    
      <h2 class="year">2017</h2>
      <ol class="bibliography">
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/tip2017.jpg" alt="Graph Regularized Auto-encoders for Image Representation" style="width: 100%;">
    </div>
    
  </div>

  <div id="liao2017graph" class="col-md-9">
    
      
        <div class="title">Graph Regularized Auto-encoders for Image Representation</div>
      
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://person.zju.edu.cn/en/yongliu" target="_blank" rel="noopener noreferrer">Liu, Yong</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Trans. on  Image Processing (TIP)</em>
      
      
      
        2017
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/document/7556994" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Image representation has been intensively explored in the domain of computer vision for its significant influence on the relative tasks such as image clustering and classification. It is valuable to learn a low-dimensional representation of an image which preserves its inherent information from the original image space. At the perspective of manifold learning, this is implemented with the local invariant idea to capture the intrinsic low-dimensional manifold embedded in the high-dimensional input space. Inspired by the recent successes of deep architectures, we propose a local invariant deep nonlinear mapping algorithm, called graph regularized auto-encoder (GAE). With the graph regularization, the proposed method preserves the local connectivity from the original image space to the representation space, while the stacked auto-encoders provide explicit encoding model for fast inference and powerful expressive capacity for complex modeling. Theoretical analysis shows that the graph regularizer penalizes the weighted Frobenius norm of the Jacobian matrix of the encoder mapping, where the weight matrix captures the local property in the input space. Furthermore, the underlying effects on the hidden representation space are revealed, providing insightful explanation to the advantage of the proposed method. Finally, the experimental results on both clustering and classification tasks demonstrate the effectiveness of our GAE as well as the correctness of the proposed theoretical analysis, and it also suggests that GAE is a superior solution to the current deep representation learning techniques comparing with variant auto-encoders and existing local invariant methods.
</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/tcds2017.jpg" alt="Place Classification with a Graph Regularized Deep Neural Network" style="width: 100%;">
    </div>
    
  </div>

  <div id="liao2017place" class="col-md-9">
    
      
        <div class="title">Place Classification with a Graph Regularized Deep Neural Network</div>
      
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.uts.edu.au/staff/sarath.kodagoda" target="_blank" rel="noopener noreferrer">Kodagoda, Sarath</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://au.linkedin.com/in/lei-shi-342b152b" target="_blank" rel="noopener noreferrer">Shi, Lei</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://person.zju.edu.cn/en/yongliu" target="_blank" rel="noopener noreferrer">Liu, Yong</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Trans. on Cognitive and Developmental Systems (TCDS)</em>
      
      
      
        2017
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/document/7501830" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Place classification is a fundamental ability that a robot should possess to carry out effective human-robot interactions. In recent years, there is a high exploitation of artificial intelligence algorithms in robotics applications. Inspired by the recent successes of deep learning methods, we propose an end-to-end learning approach for the place classification problem. With deep architectures, this methodology automatically discovers features and contributes in general to higher classification accuracies. The pipeline of our approach is composed of three parts. First, we construct multiple layers of laser range data to represent the environment information in different levels of granularity. Second, each layer of data are fed into a deep neural network for classification, where a graph regularization is imposed to the deep architecture for keeping local consistency between adjacent samples. Finally, the predicted labels obtained from all layers are fused based on confidence trees to maximize the overall confidence. Experimental results validate the effectiveness of our end-to-end place classification framework in which both the multilayer structure and the graph regularization promote the classification performance. Furthermore, results show that the features automatically learned from the raw input range data can achieve competitive results to the features constructed based on statistical and geometrical information.
</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
<li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/icra2017.jpg" alt="Parse Geometry from a Line: Monocular Depth Estimation with Partial Laser Observation" style="width: 100%;">
    </div>
    
  </div>

  <div id="liao2017parse" class="col-md-9">
    
      
        <div class="title">Parse Geometry from a Line: Monocular Depth Estimation with Partial Laser Observation</div>
      
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://scholar.google.com/citations?user=F2e_jZMAAAAJ" target="_blank" rel="noopener noreferrer">Huang, Lichao</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.uts.edu.au/staff/sarath.kodagoda" target="_blank" rel="noopener noreferrer">Kodagoda, Sarath</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://www.cbsr.ia.ac.cn/users/ynyu/" target="_blank" rel="noopener noreferrer">Yu, Yinan</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://person.zju.edu.cn/en/yongliu" target="_blank" rel="noopener noreferrer">Liu, Yong</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of the IEEE International Conf. on Robotics and Automation (ICRA)</em>
      
      
      
        2017
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/1611.02174.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Many standard robotic platforms are equipped with at least a fixed 2D laser range finder and a monocular camera. Although those platforms do not have sensors for 3D depth sensing capability, knowledge of depth is an essential part in many robotics activities. Therefore, recently, there is an increasing interest in depth estimation using monocular images. As this task is inherently ambiguous, the data-driven estimated depth might be unreliable in robotics applications. In this paper, we have attempted to improve the precision of monocular depth estimation by introducing 2D planar observation from the remaining laser range finder without extra cost. Specifically, we construct a dense reference map from the sparse laser range data, redefining the depth estimation task as estimating the distance between the real and the reference depth. To solve the problem, we construct a novel residual of residual neural network, and tightly combine the classification and regression losses for continuous depth estimation. Experimental results suggest that our method achieves considerable promotion compared to the state-of-the-art methods on both NYUD2 and KITTI, validating the effectiveness of our method on leveraging the additional sensory information. We further demonstrate the potential usage of our method in obstacle avoidance where our methodology provides comprehensive depth information compared to the solution using monocular camera or 2D laser range finder alone.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li>
</ol>
    
      <h2 class="year">2016</h2>
      <ol class="bibliography"><li>
<div class="row">
  <div class="col-md-3">
    
    <div class="img-fluid rounded">
      <img src="/assets/teaser/icra2016.jpg" alt="Understand Scene Categories by Objects: A Semantic Regularized Scene Classifier Using Convolutional Neural Networks" style="width: 100%;">
    </div>
    
  </div>

  <div id="liao2016understand" class="col-md-9">
    
      
        <div class="title">Understand Scene Categories by Objects: A Semantic Regularized Scene Classifier Using Convolutional Neural Networks</div>
      
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Liao, Yiyi</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.uts.edu.au/staff/sarath.kodagoda" target="_blank" rel="noopener noreferrer">Kodagoda, Sarath</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ywang-zju.github.io/" target="_blank" rel="noopener noreferrer">Wang, Yue</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://au.linkedin.com/in/lei-shi-342b152b" target="_blank" rel="noopener noreferrer">Shi, Lei</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://person.zju.edu.cn/en/yongliu" target="_blank" rel="noopener noreferrer">Liu, Yong</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of the IEEE International Conf. on Robotics and Automation (ICRA)</em>
      
      
      
        2016
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://arxiv.org/pdf/1509.06470.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Scene classification is a fundamental perception task for environmental understanding in today’s robotics. In this paper, we have attempted to exploit the use of popular machine learning technique of deep learning to enhance scene understanding, particularly in robotics applications. As scene images have larger diversity than the iconic object images, it is more challenging for deep learning methods to automatically learn features from scene images with less samples. Inspired by human scene understanding based on object knowledge, we address the problem of scene classification by encouraging deep neural networks to incorporate object-level information. This is implemented with a regularization of semantic segmentation. With only 5 thousand training images, as opposed to 2.5 million images, we show the proposed deep architecture achieves superior scene classification results to the state-of-the-art on a publicly available SUN RGB-D dataset. In addition, performance of semantic segmentation, the regularizer, also reaches a new record with refinement derived from predicted scene labels. Finally, we apply our SUN RGB-D dataset trained model to a mobile robot captured images to classify scenes in our university demonstrating the generalization ability of the proposed algorithm.
</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>

</li></ol>
    
    
    </div>

  </article>

</div>


    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    © Copyright 2025 Yiyi  Liao.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

    
    
    Last updated: December 17, 2025.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
