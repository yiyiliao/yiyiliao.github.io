@STRING{CVPR = {Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)}}
@STRING{ECCV = {Proc. of the European Conf. on Computer Vision (ECCV)}}
@STRING{ICCV = {Proc. of the IEEE International Conf. on Computer Vision (ICCV)}}
@STRING{ICRA = {Proc. of the IEEE International Conf. on Robotics and Automation (ICRA)}}
@STRING{THREEDV = {Proc. of the International Conf. on 3D Vision (3DV)}}
@STRING{RAL = {IEEE Robotics and Automation Letters (RA-L)}}
@STRING{TIP = {IEEE Trans. on  Image Processing (TIP)}}
@STRING{TPAMI = {IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI)}}
@STRING{NEURIPS = {Advances in Neural Information Processing Systems (NeurIPS)}}
@STRING{ARXIV = {arXiv.org}}

@ARTICLE{Fu2025TPAMI,
  author = {Fu, Xiao and Zhang, Shangzhan and Chen, Tianrun and Lu, Yichong and Zhou, Xiaowei and Geiger, Andreas and Liao, Yiyi<sup>&#x0266F;</sup>},
  title = {PanopticNeRF-360: Panoramic 3D-to-2D Label Transfer in Urban Scenes},
  journal = TPAMI,
  year = {2025},
  abstract = {Training perception systems for self-driving cars requires substantial 2D annotations that are labor-intensive to manual label. While existing datasets provide rich annotations on pre-recorded sequences, they fall short in labeling rarely encountered viewpoints, potentially hampering the generalization ability for perception models. In this paper, we present PanopticNeRF-360, a novel approach that combines coarse 3D annotations with noisy 2D semantic cues to generate high-quality panoptic labels and images from any viewpoint. Our key insight lies in exploiting the complementarity of 3D and 2D priors to mutually enhance geometry and semantics. Specifically, we propose to leverage coarse 3D bounding primitives and noisy 2D semantic and instance predictions to guide geometry optimization, by encouraging predicted labels to match panoptic pseudo ground truth. Simultaneously, the improved geometry assists in filtering 3D\&2D annotation noise by fusing semantics in 3D space via a learned semantic field. To further enhance appearance, we combine MLP and hash grids to yield hybrid scene features, striking a balance between high-frequency appearance and contiguous semantics. Our experiments demonstrate PanopticNeRF-360's state-of-the-art performance over label transfer methods on the challenging urban scenes of the KITTI-360 dataset. Moreover, PanopticNeRF-360 enables omnidirectional rendering of high-fidelity, multi-view and spatiotemporally consistent appearance, semantic and instance labels.},
  pdf = {https://arxiv.org/pdf/2309.10815},
  code = {https://github.com/fuxiao0719/PanopticNeRF/tree/panopticnerf360},
  teaser = {Fu2025TPAMI.gif},
  website = {https://fuxiao0719.github.io/projects/panopticnerf360/},
  selected = True,
}


@inproceedings{Li2025CVPR,
  title = {GIFStream: 4D Gaussian-based Immersive Video with Feature Stream},
  author = {Li, Hao and Li, Sicheng and Gao, Xiang and Batuer, Abudouaihati and Yu, Lu<sup>&#x0266F;</sup> and Liao, Yiyi<sup>&#x0266F;</sup>},
  booktitle = CVPR,
  year = {2025},
  abstract = {Immersive video offers a 6-Dof-free viewing experience, potentially playing a key role in future video technology. Recently, 4D Gaussian Splatting has gained attention as an effective approach for immersive video due to its high rendering efficiency and quality, though maintaining quality with manageable storage remains challenging. To address this, we introduce GIFStream, a novel 4D Gaussian representation using a canonical space and a deformation field enhanced with time-dependent feature streams. These feature streams enable complex motion modeling and allow efficient compression by leveraging their motion-awareness and temporal correspondence. Additionally, we incorporate both temporal and spatial compression networks for end-to-end compression. Experimental results show that GIFStream delivers high-quality immersive video at 30 Mbps, with real-time rendering and fast decoding on an RTX 4090.},
  teaser = {Li2025CVPR.jpg},
  pdf = {https://arxiv.org/pdf/2505.07539},
  website = {https://xdimlab.github.io/GIFStream/},
  selected = True,
}

@inproceedings{Shao2025CVPR,
  title = {Learning temporally consistent video depth from video diffusion priors},
  author = {Shao, Jiahao and Yang, Yuanbo and Zhou, Hongyu and Zhang, Youmin and Shen, Yujun and Guizilini, Vitor and Wang, Yue2 and Poggi, Matteo and Liao, Yiyi<sup>&#x0266F;</sup>},
  booktitle = CVPR,
  year = {2025},
  abstract = {This work addresses the challenge of streamed video depth estimation, which expects not only per-frame accuracy but, more importantly, cross-frame consistency. We argue that sharing contextual information between frames or clips is pivotal in fostering temporal consistency. Thus, instead of directly developing a depth estimator from scratch, we reformulate this predictive task into a conditional generation problem to provide contextual information within a clip and across clips. Specifically, we propose a consistent context-aware training and inference strategy for arbitrarily long videos to provide cross-clip context. We sample independent noise levels for each frame within a clip during training while using a sliding window strategy and initializing overlapping frames with previously predicted frames without adding noise. Moreover, we design an effective training strategy to provide context within a clip. Extensive experimental results validate our design choices and demonstrate the superiority of our approach, dubbed ChronoDepth.},
  teaser = {Shao2025CVPR.gif},
  pdf = {https://arxiv.org/pdf/2406.01493},
  website = {https://xdimlab.github.io/ChronoDepth/},
  selected = True,
}

@inproceedings{Lu2025CVPR,
  title = {UrbanCAD: Towards Highly Controllable and Photorealistic 3D Vehicles for Urban Scene Simulation},
  author = {Lu, Yichong and Cai, Yichi and Zhang, Shangzhan and Zhou, Hongyu and Hu, Haoji and Yu, Huimin and Geiger, Andreas and Liao, Yiyi<sup>&#x0266F;</sup>},
  booktitle = CVPR,
  year = {2025},
  abstract = {Photorealistic 3D vehicle models with high controllability are essential for autonomous driving simulation and data augmentation. While handcrafted CAD models provide flexible controllability, free CAD libraries often lack the high-quality materials necessary for photorealistic rendering. Conversely, reconstructed 3D models offer high-fidelity rendering but lack controllability. In this work, we introduce UrbanCAD, a framework that pushes the frontier of the photorealism-controllability trade-off by generating highly controllable and photorealistic 3D vehicle digital twins from a single urban image and a collection of free 3D CAD models and handcrafted materials. These digital twins enable realistic 360-degree rendering, vehicle insertion, material transfer, relighting, and component manipulation such as opening doors and rolling down windows, supporting the construction of long-tail scenarios. To achieve this, we propose a novel pipeline that operates in a retrieval-optimization manner, adapting to observational data while preserving flexible controllability and fine-grained handcrafted details. Furthermore, given multi-view background perspective and fisheye images, we approximate environment lighting using fisheye images and reconstruct the background with 3DGS, enabling the photorealistic insertion of optimized CAD models into rendered novel view backgrounds. Experimental results demonstrate that UrbanCAD outperforms baselines based on reconstruction and retrieval in terms of photorealism. Additionally, we show that various perception models maintain their accuracy when evaluated on UrbanCAD with in-distribution configurations but degrade when applied to realistic out-of-distribution data generated by our method. This suggests that UrbanCAD is a significant advancement in creating photorealistic, safety-critical driving scenarios for downstream applications.},
  teaser = {Lu2025CVPR.jpg},
  website = {https://xdimlab.github.io/UrbanCAD/},
  pdf = {https://arxiv.org/pdf/2411.19292},
  selected = True,
}

@inproceedings{Miao2025CVPR,
  title = {EVolSplat: Efficient Volume-based Gaussian Splatting for Urban View Synthesis},
  author = {Miao, Sheng and Huang, Jiaxin and Bai, Dongfeng and Yan, Xu and Zhou, Hongyu and Wang, Yue and Liu, Bingbing and Geiger, Andreas and Liao, Yiyi<sup>&#x0266F;</sup>},
  booktitle = CVPR,
  year = {2025},
  abstract = {Novel view synthesis of urban scenes is essential for autonomous driving-related applications. Existing NeRF and 3DGS-based methods show promising results in achieving photorealistic renderings but require slow, per-scene optimization. We introduce EVolSplat, an efficient 3D Gaussian Splatting model for urban scenes that works in a feed-forward manner. Unlike existing feed-forward, pixel-aligned 3DGS methods, which often suffer from issues like multi-view inconsistencies and duplicated content, our approach predicts 3D Gaussians across multiple frames within a unified volume using a 3D convolutional network. This is achieved by initializing 3D Gaussians with noisy depth predictions, and then refining their geometric properties in 3D space and predicting color based on 2D textures. Our model also handles distant views and the sky with a flexible hemisphere background model. This enables us to perform fast, feed-forward reconstruction while achieving real-time rendering. Experimental evaluations on the KITTI-360 and Waymo datasets show that our method achieves state-of-the-art quality compared to existing feed-forward 3DGS- and NeRF-based methods. },
  teaser = {Miao2025CVPR.gif},
  pdf = {https://arxiv.org/pdf/2503.20168},
  website = {https://xdimlab.github.io/EVolSplat/},
  selected = True,
}


@inproceedings{Yang2025CVPR,
  title = {Prometheus: 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D Scene Generation},
  author = {Yang, Yuanbo and Shao, Jiahao and Li, Xinyang and Shen, Yujun and Geiger, Andreas and Liao, Yiyi<sup>&#x0266F;</sup>},
  booktitle = CVPR,
  year = {2025},
  abstract = {In this work, we introduce Prometheus, a 3D-aware latent diffusion model for text-to-3D generation at both object and scene levels in seconds. We formulate 3D scene generation as multi-view, feed-forward, pixel-aligned 3D Gaussian generation within the latent diffusion paradigm. To ensure generalizability, we build our model upon pre-trained text-to-image generation model with only minimal adjustments, and further train it using a large number of images from both single-view and multi-view datasets. Furthermore, we introduce an RGB-D latent space into 3D Gaussian generation to disentangle appearance and geometry information, enabling efficient feed-forward generation of 3D Gaussians with better fidelity and geometry. Extensive experimental results demonstrate the effectiveness of our method in both feed-forward 3D Gaussian reconstruction and text-to-3D generation.},
  teaser = {Yang2025CVPR.jpg},
  pdf = {https://arxiv.org/pdf/2412.21117},
  website = {https://freemty.github.io/project-prometheus/},
  selected = True,
}

@inproceedings{Wu2024RAL,
  title = {DORec: Decomposed Object Reconstruction and Segmentation Utilizing 2D Self-Supervised Features},
  author = {Wu, Jun and Li, Sicheng and Ji, Sihui and Yang, Yifei and  Wang, Yue and Xiong, Rong<sup>&#x0266F;</sup> and Liao, Yiyi<sup>&#x0266F;</sup>},
  booktitle = RAL,
  year = {2024},
  abstract = {Recovering 3D geometry and textures of individual objects is crucial for many robotics applications, such as manipulation, pose estimation, and autonomous driving. However, decomposing a target object from a complex background is challenging. Most existing approaches rely on costly manual labels to acquire object instance perception. Recent advancements in 2D self-supervised learning offer new prospects for identifying objects of interest, yet leveraging such noisy 2D features for clean decomposition remains difficult. In this paper, we propose a Decomposed Object Reconstruction (DORec) network based on neural implicit representations. Our key idea is to use 2D self-supervised features to create two levels of masks for supervision: a binary mask for foreground regions and a K-cluster mask for semantically similar regions. These complementary masks result in robust decomposition. Experimental results on different datasets show DORec's superiority in segmenting and reconstructing diverse foreground objects from varied backgrounds enabling downstream tasks such as pose estimation.},
  teaser = {Wu2024RAL.jpg},
  pdf = {https://arxiv.org/pdf/2310.11092.pdf},
  selected = True,
}


@inproceedings{Chen2024ECCV,
  title = {TeFF: Learning 3D-Aware GANs from Unposed Images with Template Feature Field},
  author = {Chen, Xinya and Guo, Hanlei and Bin, Yanrui and Zhang, Shangzhan and Yang, Yuanbo and Wang, Yue and Shen, Yujun and Liao, Yiyi<sup>&#x0266F;</sup>},
  booktitle = ECCV,
  year = {2024},
  abstract = {Collecting accurate camera poses of training images has been shown to well serve the learning of 3D-aware generative adversarial networks (GANs) yet can be quite expensive in practice. This work targets learning 3D-aware GANs from unposed images, for which we propose to perform on-the-fly pose estimation of training images with a learned template feature field (TeFF). Concretely, in addition to a generative radiance field as in previous approaches, we ask the generator to also learn a field from 2D semantic features while sharing the density from the radiance field. Such a framework allows us to acquire a canonical 3D feature template leveraging the dataset mean discovered by the generative model, and further efficiently estimate the pose parameters on real data. Experimental results on various challenging datasets demonstrate the superiority of our approach over state-of-the-art alternatives from both the qualitative and the quantitative perspectives.},
  pdf = {https://arxiv.org/pdf/2404.05705.pdf},
  award = {(Oral)},
  teaser = {Chen2024ECCV.gif},
  website = {https://xdimlab.github.io/TeFF/},
  code = {https://github.com/XinyaChen21/TeFF/},
  selected = True,
}

@inproceedings{Ji2024ECCV,
  title = {REFRAME: REFlective Surface ReAl-Time Rendering for MobilE Devices},
  author = {Ji, Chaojie and Li, Yufeng and Liao, Yiyi<sup>&#x0266F;</sup>},
  booktitle = ECCV,
  year = {2024},
  abstract = {This work tackles the challenging task of achieving real-time novel view synthesis for reflective surfaces across various scenes. Existing real-time rendering methods, especially those based on meshes, often have subpar performance in modeling surfaces with rich view-dependent appearances. Our key idea lies in leveraging meshes for rendering acceleration while incorporating a novel approach to parameterize view-dependent information. We decompose the color into diffuse and specular, and model the specular color in the reflected direction based on a neural environment map. Our experiments demonstrate that our method achieves comparable reconstruction quality for highly reflective surfaces compared to state-of-the-art offline methods, while also efficiently enabling real-time rendering on edge devices such as smartphones.},
  pdf = {https://arxiv.org/pdf/2403.16481.pdf},
  teaser = {Ji2024ECCV.jpg},
  website = {https://xdimlab.github.io/REFRAME/},
  code = {https://github.com/MARVELOUSJI/REFRAME},
  selected = True,
}

@inproceedings{Miao2024ECCV,
  title = {EDUS: Efficient Depth-Guided Urban View Synthesis},
  author = {Miao, Sheng<sup>*</sup> and Huang, Jiaxin<sup>*</sup> and Dongfeng, Bai and Qiu, Weichao and Liu, Bingbing and Geiger, Andreas and Liao, Yiyi<sup>&#x0266F;</sup>},
  booktitle = ECCV,
  year = {2024},
  abstract = {Recent advances in implicit scene representation enable high- fidelity street view novel view synthesis. However, existing methods op- timize a neural radiance field for each scene, relying heavily on dense training images and extensive computation resources. To mitigate this shortcoming, we introduce a new method called Efficient Depth-Guided Urban View Synthesis (EDUS) for fast feed-forward inference and effi- cient per-scene fine-tuning. Different from prior generalizable methods that infer geometry based on feature matching, EDUS leverages noisy predicted geometric priors as guidance to enable generalizable urban view synthesis from sparse input images. The geometric priors allow us to ap- ply our generalizable model directly in the 3D space, gaining robustness across various sparsity levels. Through comprehensive experiments on the KITTI-360 and Waymo datasets, we demonstrate promising gener- alization abilities on novel street scenes. Moreover, our results indicate that EDUS achieves state-of-the-art performance in sparse view settings when combined with fast test-time optimization.},
  pdf = {https://arxiv.org/pdf/2407.12395.pdf},
  teaser = {Miao2024ECCV.gif},
  website = {https://xdimlab.github.io/EDUS/},
  code = {https://github.com/Miaosheng1/EDUS},
  selected = True,
}

@inproceedings{Zhou2024CVPR,
  title = {HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting},
  author = {Zhou, Hongyu and Shao, Jiahao and Xu, Lu and Dongfeng, Bai and Qiu, Weichao and Liu, Bingbing and Wang, Yue and Geiger, Andreas and Liao, Yiyi<sup>&#x0266F;</sup>},
  booktitle = CVPR,
  year = {2024},
  abstract = {Holistic understanding of urban scenes based on RGB images is a challenging yet important problem. It encompasses understanding both the geometry and appearance to enable novel view synthesis, parsing semantic labels, and tracking moving objects. Despite considerable progress, existing approaches often focus on specific aspects of this task and require additional inputs such as LiDAR scans or manually annotated 3D bounding boxes. In this paper, we introduce a novel pipeline that utilizes 3D Gaussian Splatting for holistic urban scene understanding. Our main idea involves the joint optimization of geometry, appearance, semantics, and motion using a combination of static and dynamic 3D Gaussians, where moving object poses are regularized via physical constraints. Our approach offers the ability to render new viewpoints in real-time, yielding 2D and 3D semantic information with high accuracy, and reconstruct dynamic scenes, even in scenarios where 3D bounding box detection are highly noisy. Experimental results on KITTI, KITTI-360, and Virtual KITTI 2 demonstrate the effectiveness of our approach.},
  pdf = {https://arxiv.org/pdf/2403.12722.pdf},
  teaser = {Zhou2024CVPR.gif},
  website = {https://xdimlab.github.io/hugs_website/},
  code = {https://github.com/hyzhou404/HUGS},
  selected = True,
}

@inproceedings{Li2024CVPR,
  title = {NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation},
  author = {Li, Sicheng and Li, Hao and Liao, Yiyi<sup>&#x0266F;</sup> and Yu, Lu<sup>&#x0266F;</sup>},
  booktitle = CVPR,
  year = {2024},
  abstract = {The emergence of Neural Radiance Fields (NeRF) has greatly impacted 3D scene modeling and novel-view synthesis. As a kind of visual media for 3D scene representation, compression with high rate-distortion performance is an eternal target. Motivated by advances in neural compression and neural field representation, we propose NeRFCodec, an end-to-end NeRF compression framework that integrates non-linear transform, quantization, and entropy coding for memory-efficient scene representation. Since training a non-linear transform directly on a large scale of NeRF feature planes is impractical, we discover that pre-trained neural 2D image codec can be utilized for compressing the features when adding content-specific parameters. Specifically, we reuse neural 2D image codec but modify its encoder and decoder heads, while keeping the other parts of the pre-trained decoder frozen. This allows us to train the full pipeline via supervision of rendering loss and entropy loss, yielding the rate-distortion balance by updating the content-specific parameters. At test time, the bitstreams containing latent code, feature decoder head, and other side information are transmitted for communication. Experimental results demonstrate our method outperforms existing NeRF compression methods, enabling high-quality novel view synthesis with a memory budget of 0.5 MB.},
  pdf = {https://arxiv.org/pdf/2404.02185.pdf},
  teaser = {Li2024CVPR.jpg},
  website = {https://jasonlsc.github.io/nerfcodec_homepage/},
  code = {https://github.com/JasonLSC/NeRFCodec_public},
  selected = True,
}

@inproceedings{Yunus2024CGF,
  title={Recent Trends in 3D Reconstruction of General Non-Rigid Scenes},
  author={Yunus, Raza and Lenssen, Jan Eric and Niemeyer, Michael and Liao, Yiyi and Rupprecht, Christian and Theobalt, Christian and Pons-Moll, Gerard and Huang, Jia-Bin and Golyanik, Vladislav and Ilg, Eddy},
  booktitle={State-of-the-Art Report at EUROGRAPHICS},
  year={2024},
  teaser = {Yunus2024CGF.jpg},
  selected = True,
  abstract = {Reconstructing models of the real world, including 3D geometry, appearance, and motion of real scenes, is essential for computer graphics and computer vision. It enables the synthesizing of photorealistic novel views, useful for the movie industry and AR/VR applications. It also facilitates the content creation necessary in computer games and AR/VR by avoiding laborious manual design processes. Further, such models are fundamental for intelligent computing systems that need to interpret real-world scenes and actions to act and interact safely with the human world. Notably, the world surrounding us is dynamic, and reconstructing models of dynamic, non-rigidly moving scenes is a severely underconstrained and challenging problem.  This state-of-the-art report (STAR) offers the reader a comprehensive summary of state-of-the-art techniques with monocular and multi-view inputs such as data from RGB and RGB-D sensors, among others, conveying an understanding of different approaches, their potential applications, and promising further research directions. The report covers 3D reconstruction of general non-rigid scenes and further addresses the techniques for scene decomposition, editing and controlling, and generalizable and generative modeling. More specifically, we first review the common and fundamental concepts necessary to understand and navigate the field and then discuss the state-of-the-art techniques by reviewing recent approaches that use traditional and machine-learning-based neural representations, including a discussion on the newly enabled applications. The STAR is con- cluded with a discussion of the remaining limitations and open challenges.},
  pdf = {https://arxiv.org/pdf/2403.15064.pdf}
}


@inproceedings{Mao2024ICRA,
  title = {NGEL-SLAM: Neural Implicit Representation-based Global Consistent Low-Latency SLAM System},
  author = {Mao, Yunxuan and Yu, Xuan and Wang, Kai and Wang, Yue and Xiong, Rong and Liao, Yiyi<sup>&#x0266F;</sup>},
  booktitle = ICRA,
  year = {2024},
  award = {(Best Robot Vision Paper)},
  abstract = {Neural implicit representations have emerged as a promising solution for providing dense geometry in Simultaneous Localization and Mapping (SLAM). However, existing methods in this direction fall short in terms of global consistency and low latency. This paper presents NGEL-SLAM to tackle the above challenges. To ensure global consistency, our system leverages a traditional feature-based tracking module that incorporates loop closure. Additionally, we maintain a global consistent map by representing the scene using multiple neural implicit fields, enabling quick adjustment to the loop closure.  Moreover, our system allows for fast convergence through the use of octree-based implicit representations. The combination of rapid response to loop closure and fast convergence makes our system a truly low-latency system that achieves global consistency. Our system enables rendering high-fidelity RGBD images, along with extracting dense and complete surfaces.  Experiments on both synthetic and real-world datasets suggest that our system achieves state-of-the-art tracking and mapping accuracy while maintaining low latency.},
  teaser = {Mao2024ICRA.jpg},
  selected = True,
  pdf = {https://arxiv.org/pdf/2311.09525.pdf},
  code = {https://github.com/YunxuanMao/ngel_slam},
}


@inproceedings{Shi2023Arxiv,
  title={3D Generative Models: A Survey},
  author={Shi, Zifan<sup>*</sup> and Peng, Sida<sup>*</sup> and Xu, Yinghao<sup>*</sup> and Geiger, Andreas and Liao, Yiyi<sup>&#x0266F;</sup> and Shen, Yujun<sup>&#x0266F;</sup>},
  booktitle=ARXIV,
  year={2023},
  abstract={Generative models aim to learn the distribution of observed data by generating new instances. With the advent of neural networks, deep generative models, including variational autoencoders (VAEs), generative adversarial networks (GANs), and diffusion models (DMs), have progressed remarkably in synthesizing 2D images. Recently, researchers started to shift focus from 2D to 3D space, considering that 3D data is more closely aligned with our physical world and holds immense practical potential. However, unlike 2D images, which possess an inherent and efficient representation (i.e., a pixel grid), representing 3D data poses significantly greater challenges. Ideally, a robust 3D representation should be capable of accurately modeling complex shapes and appearances while being highly efficient in handling high-resolution data with high processing speeds and low memory requirements. Regrettably, existing 3D representations, such as point clouds, meshes, and neural fields, often fail to satisfy all of these requirements simultaneously.  In this survey, we thoroughly review the ongoing developments of 3D generative models, including methods that employ 2D and 3D supervision. Our analysis centers on generative models, with a particular focus on the representations utilized in this context. We believe our survey will help the community to track the field’s evolution and to spark innovative ideas to propel progress towards solving this challenging task.},
  pdf={https://arxiv.org/pdf/2210.15663.pdf},
  teaser={Shi2023Arxiv.jpg}
}


@inproceedings{Yu2023RAL,
  title = {NF-Atlas: Multi-Volume Neural Feature Fields for Large Scale LiDAR Mapping},
  author = {Yu, Xuan and Liu, Yili and Mao, Sitong and Zhou, Shunbo and Xiong, Rong and Liao, Yiyi and Wang, Yue},
  booktitle = RAL,
  year = {2023},
  abstract={ LiDAR Mapping has been a long-standing problem in robotics. Recent progress in neural implicit representation has brought new opportunities to robotic mapping. In this paper, we propose the multi-volume neural feature fields, called NF-Atlas, which bridge the neural feature volumes with pose graph optimization. By regarding the neural feature volume as pose graph nodes and the relative pose between volumes as pose graph edges, the entire neural feature field becomes both locally rigid and globally elastic. Locally, the neural feature volume employs a sparse feature Octree and a small MLP to encode the signed distance function (SDF) of the submap with an option of semantics. Learning the map using this structure allows for end-to-end solving of maximum a posteriori (MAP) based probabilistic mapping. Globally, the map is built volume by volume independently, avoiding catastrophic forgetting when mapping incrementally. Furthermore, when a loop closure occurs, with the elastic pose graph based representation, only updating the origin of neural volumes is required without remapping. Finally, these functionalities of NF-Atlas are validated. Thanks to the sparsity and the optimization based formulation, NF-Atlas shows competitive performance in terms of accuracy, efficiency and memory usage on both simulation and real-world datasets.  },
  teaser = {Yu2023RAL.jpg},
  selected = True,
  pdf = {https://arxiv.org/pdf/2304.04624.pdf},
  code = {https://github.com/yuxuan1206/NF-Atlas},
  website = {https://yuxuan1206.github.io/NFAtlas/}
}

@inproceedings{Yang2023ICCV,
  title = {UrbanGIRAFFE: Representing Urban Scenes as Compositional Generative Neural Feature Fields},
  author = {Yang, Yuanbo and Yang, Yifei and Guo, Hanlei and Xiong, Rong and Wang, Yue and Liao, Yiyi<sup>&#x0266F;</sup>},
  booktitle = ICCV,
  year = {2023},
  abstract = {Generating photorealistic images with controllable camera pose and scene contents is essential for many applications including AR/VR and simulation. Despite the fact that rapid progress has been made in 3D-aware generative models, most existing methods focus on object-centric images and are not applicable to generating urban scenes for free camera viewpoint control and scene editing. To address this challenging task, we propose UrbanGIRAFFE, which uses a coarse 3D panoptic prior, including the layout distribution of uncountable stuff and countable objects, to guide a 3D-aware generative model. Our model is compositional and controllable as it breaks down the scene into stuff, objects, and sky. Using stuff prior in the form of semantic voxel grids, we build a conditioned stuff generator that effectively incorporates the coarse semantic and geometry information. The object layout prior further allows us to learn an object generator from cluttered scenes. With proper loss functions, our approach facilitates photorealistic 3D-aware image synthesis with diverse controllability, including large camera movement, stuff editing, and object manipulation. We validate the effectiveness of our model on both synthetic and real-world datasets, including the challenging KITTI-360 dataset.},
  pdf = {https://arxiv.org/pdf/2303.14167.pdf},
  teaser = {Yang2023ICCV.gif},
  website = {https://lv3d.github.io/urbanGIRAFFE/},
  code = {https://github.com/freemty/urbanGIRAFFE},
  selected = True,
}

@inproceedings{Chen2023ICCV,
  title = {VeRi3D: Generative Vertex-based Radiance Fields for 3D Controllable Human Image Synthesis},
  author = {Chen, Xinya and Huang, Jiaxin and Bin, Yanrui and Yu, Lu and Liao, Yiyi<sup>&#x0266F;</sup>},
  booktitle = ICCV,
  year = {2023},
  abstract = {Unsupervised learning of 3D-aware generative adversarial networks has lately made much progress. Some recent work demonstrates promising results of learning human generative models using neural articulated radiance fields, yet their generalization ability and controllability lag behind parametric human models, i.e., they do not perform well when generalizing to novel pose/shape and are not part controllable. To solve these problems, we propose VeRi3D, a generative human vertex-based radiance field parameterized by vertices of the parametric human template, SMPL. We map each 3D point to the local coordinate system defined on its neighboring vertices, and use the corresponding vertex feature and local coordinates for mapping it to color and density values. We demonstrate that our simple approach allows for generating photorealistic human images with free control over camera pose, human pose, shape, as well as enabling part-level editing.},
  pdf = {https://arxiv.org/pdf/2303.14167.pdf},
  teaser = {Chen2023ICCV.jpg},
  website = {https://xdimlab.github.io/VeRi3d/},
  code = {https://github.com/XinyaChen21/Veri3d},
  selected = True,
}

@inproceedings{Li2023ICCV,
  title = {RICO: Regularizing the Unobservable for Indoor Compositional Reconstruction},
  author = {Li, Zizhang and Lyu, Xiaoyang and Ding, Yuanyuan and Wang, Mengmeng and Liao, Yiyi<sup>&#x0266F;</sup> and Liu, Yong<sup>&#x0266F;</sup>},
  booktitle = ICCV,
  year = {2023},
  abstract = {Recently, neural implicit surfaces have become popular for multi-view reconstruction. To facilitate practical applications like scene editing and manipulation, some works extend the framework with semantic masks input for the object-compositional reconstruction rather than the holistic perspective. Though achieving plausible disentanglement, the performance drops significantly when processing the indoor scenes where objects are usually partially observed. We propose RICO to address this by regularizing the unobservable regions for indoor compositional reconstruction. Our key idea is to first regularize the smoothness of the occluded background, which then in turn guides the foreground object reconstruction in unobservable regions based on the object-background relationship. Particularly, we regularize the geometry smoothness of occluded background patches. With the improved background surface, the signed distance function and the reversedly rendered depth of objects can be optimized to bound them within the background range. Extensive experiments show our method outperforms other methods on synthetic and real-world indoor scenes and prove the effectiveness of proposed regularizations.},
  pdf = {https://arxiv.org/pdf/2303.08605.pdf},
  teaser = {Li2023ICCV.jpg},
  code = {https://github.com/kyleleey/RICO},
  selected = True,
}


@inproceedings{Li2023CVPR,
  title = {SteerNeRF: Accelerating NeRF Rendering via Smooth Viewpoint Trajectory},
  author = {Li, Sicheng and Li, Hao and Wang, Yue and Liao, Yiyi<sup>&#x0266F;</sup> and Yu, Lu<sup>&#x0266F;</sup>},
  booktitle = CVPR,
  year = {2023},
  abstract = {Neural Radiance Fields (NeRF) have demonstrated superior novel view synthesis performance but are slow at rendering. To speed up the volume rendering process, many acceleration methods have been proposed at the cost of large memory consumption. To push the frontier of the efficiency-memory trade-off, we explore a new perspective to accelerate NeRF rendering, leveraging a key fact that the viewpoint change is usually smooth and continuous in interactive viewpoint control. This allows us to leverage the information of preceding viewpoints to reduce the number of rendered pixels as well as the number of sampled points along the ray of the remaining pixels. In our pipeline, a low-resolution feature map is rendered first by volume rendering, then a lightweight 2D neural renderer is applied to generate the output image at target resolution leveraging the features of preceding and current frames. We show that the proposed method can achieve competitive rendering quality while reducing the rendering time with little memory overhead, enabling 30FPS at 1080P image resolution with a low memory footprint.},
  pdf = {https://arxiv.org/pdf/2212.08476.pdf},
  teaser = {Li2023CVPR.jpg},
  code = {https://github.com/JasonLSC/SteerNeRF_official},
  website = {https://jasonlsc.github.io/SteerNeRF/},
  selected = True,
}

@inproceedings{Shi2023CVPR,
  title = {Learning 3D-aware Image Synthesis with Unknown Pose Distribution},
  author = {Shi, Zifan and Shen, Yujun and Xu, Yinghao and Peng, Sida and Liao, Yiyi and Guo, Sheng and Chen, Qifeng and Yeung, Dit-Yan},
  booktitle = CVPR,
  year = {2023},
  abstract = {Existing methods for 3D-aware image synthesis largely depend on the 3D pose distribution pre-estimated on the training set. An inaccurate estimation may mislead the model into learning faulty geometry. This work proposes PoF3D that frees generative radiance fields from the requirements of 3D pose priors. We first equip the generator with an efficient pose learner, which is able to infer a pose from a latent code, to approximate the underlying true pose distribution automatically. We then assign the discriminator a task to learn pose distribution under the supervision of the generator and to differentiate real and synthesized images with the predicted pose as the condition. The pose-free generator and the pose-aware discriminator are jointly trained in an adversarial manner. Extensive results on a couple of datasets confirm that the performance of our approach, regarding both image quality and geometry quality, is on par with state of the art. To our best knowledge, PoF3D demonstrates the feasibility of learning high-quality 3D-aware image synthesis without using 3D pose priors for the first time.},
  pdf = {https://arxiv.org/pdf/2301.07702.pdf},
  teaser = {Shi2023CVPR.jpeg},
  website = {https://vivianszf.github.io/pof3d/},
  code = {https://github.com/VivianSZF/pof3d},
  selected = False,
}

@inproceedings{Zhang2023CVPR,
  title = {Painting 3D Nature in 2D: View Synthesis of Natural Scenes from a Single Semantic Mask},
  author={Zhang, Shangzhan and Peng, Sida and Chen, Tianrun and Mou, Linzhan and Lin, Haotong and Yu, Kaicheng and Liao, Yiyi and Zhou, Xiaowei},
  booktitle = CVPR,
  year = {2023},
  abstract = {We introduce a novel approach that takes a single semantic mask as input to synthesize multi-view consistent color images of natural scenes, trained with a collection of single images from the Internet. Prior works on 3D-aware image synthesis either require multi-view supervision or learning category-level prior for specific classes of objects, which can hardly work for natural scenes. Our key idea to solve this challenging problem is to use a semantic field as the intermediate representation, which is easier to reconstruct from an input semantic mask and then translate to a radiance field with the assistance of off-the-shelf semantic image synthesis models. Experiments show that our method outperforms baseline methods and produces photorealistic, multi-view consistent videos of a variety of natural scenes.},
  pdf = {https://arxiv.org/pdf/2302.07224.pdf},
  teaser = {Zhang2023CVPR.gif},
  website = {https://zju3dv.github.io/paintingnature/},
  code = {https://github.com/zhanghe3z/PaintingNature/blob/main/README.md},
  selected = False,
}


@inproceedings{Schwarz2022NEURIPS,
  title = {VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids},
  author = {Schwarz, Katja and Sauer, Axel and Niemeyer, Michael and Liao, Yiyi and Geiger, Andreas},
  booktitle = NEURIPS,
  year = {2022},
  abstract = {State-of-the-art 3D-aware generative models rely on coordinate-based MLPs to parameterize 3D radiance fields. While demonstrating impressive results, querying an MLP for every sample along each ray leads to slow rendering. Therefore, existing approaches often render low-resolution feature maps and process them with an upsampling network to obtain the final image. Albeit efficient, neural rendering often entangles viewpoint and content such that changing the camera pose results in unwanted changes of geometry or appearance. Motivated by recent results in voxel-based novel view synthesis, we investigate the utility of sparse voxel grid representations for fast and 3D-consistent generative modeling in this paper. Our results demonstrate that monolithic MLPs can indeed be replaced by 3D convolutions when combining sparse voxel grids with progressive growing, free space pruning and appropriate regularization. To obtain a compact representation of the scene and allow for scaling to higher voxel resolutions, our model disentangles the foreground object (modeled in 3D) from the background (modeled in 2D). In contrast to existing approaches, our method requires only a single forward pass to generate a full 3D scene. It hence allows for efficient rendering from arbitrary viewpoints while yielding 3D consistent results with high visual fidelity.
},
  pdf = {https://arxiv.org/pdf/2206.07695.pdf},
  website = {https://katjaschwarz.github.io/voxgraf},
  code = {https://github.com/autonomousvision/voxgraf},
  teaser = {voxgraf.gif},
  selected = True,
}

@inproceedings{Fu2022THREEDV,
  author = {Xiao<sup>*</sup> Fu and Shangzhan<sup>*</sup> Zhang and Tianrun Chen and Yichong Lu and Lanyun Zhu and Xiaowei Zhou and Andreas Geiger and Yiyi Liao},
  title = {Panoptic NeRF: 3D-to-2D Label Transfer for Panoptic Urban Scene Segmentation},
  booktitle = THREEDV,
  year = {2022},
  abstract = {Large-scale training data with high-quality annotations is critical for training semantic and instance segmentation models. Unfortunately, pixel-wise annotation is labor-intensive and costly, raising the demand for more efficient labeling strategies. In this work, we present a novel 3D-to-2D label transfer method, Panoptic NeRF, which aims for obtaining per-pixel 2D semantic and instance labels from easy-to-obtain coarse 3D bounding primitives. Our method utilizes NeRF as a differentiable tool to unify coarse 3D annotations and 2D semantic cues transferred from existing datasets. We demonstrate that this combination allows for improved geometry guided by semantic information, enabling rendering of accurate semantic maps across multiple views. Furthermore, this fusion process resolves label ambiguity of the coarse 3D annotations and filters noise in the 2D predictions. By inferring in 3D space and rendering to 2D labels, our 2D semantic and instance labels are multi-view consistent by design. Experimental results show that Panoptic NeRF outperforms existing label transfer methods in terms of accuracy and multi-view consistency on challenging urban scenes of the KITTI-360 dataset.},
  teaser = {3dv2022.gif},
  pdf = {https://www.cvlibs.net/publications/Fu2022THREEDV.pdf},
  poster = {https://www.cvlibs.net/publications/Fu2022THREEDV_poster.pdf},
  website = {https://fuxiao0719.github.io/projects/panopticnerf},
  code = {https://github.com/fuxiao0719/PanopticNeRF},
  video = {https://www.youtube.com/watch?v=5QKTeFLciWo},
  selected = True,
}

@inproceedings{Guo2022ECCV,
  author = {Jiaxin Guo and Fangxun Zhong and Rong Xiong and Yunhui Liu and Yue Wang and Yiyi Liao},
  title = {A Visual Navigation Perspective for Category-Level Object Pose Estimation},
  booktitle = ECCV,
  year = {2022},
  abstract = {This paper studies category-level object pose estimation based on a single monocular image. Recent advances in pose-aware generative models have paved the way for addressing this challenging task using analysis-by-synthesis. The idea is to sequentially update a set of latent variables, \eg, pose, shape, and appearance, of the generative model until the generated image best agrees with the observation. However, convergence and efficiency are two challenges of this inference procedure. In this paper, we take a deeper look at the inference of analysis-by-synthesis from the perspective of visual navigation, and investigate what is a good navigation policy for this specific task. We evaluate three different strategies, including gradient descent, reinforcement learning and imitation learning, via thorough comparisons in terms of convergence, robustness and efficiency. Moreover, we show that a simple hybrid approach leads to an effective and efficient solution. We further compare these strategies to state-of-the-art methods, and demonstrate superior performance on synthetic and real-world datasets leveraging off-the-shelf pose-aware generative models.},
  teaser = {eccv2022.jpg},
  pdf = {https://arxiv.org/pdf/2203.13572.pdf},
  code = {https://github.com/wrld/visual_navigation_pose_estimation},
  video = {https://youtu.be/bIsP7Necw0Q}
}

@inproceedings{Peng2021NEURIPS,
  author = {Songyou Peng and Chiyu<sup>&#x0266F;</sup> Jiang and Yiyi<sup>&#x0266F;</sup> Liao and Michael Niemeyer and Marc Pollefeys and Andreas Geiger},
  title = {Shape As Points: A Differentiable Poisson Solver},
  booktitle = NEURIPS,
  year = {2021},
  abstract = {In recent years, neural implicit representations gained popularity in 3D reconstruction due to their expressiveness and flexibility. However, the implicit nature of neural implicit representations results in slow inference times and requires careful initialization. In this paper, we revisit the classic yet ubiquitous point cloud representation and introduce a differentiable point-to-mesh layer using a differentiable formulation of Poisson Surface Reconstruction (PSR) which allows for a GPU-accelerated fast solution of the indicator function given an oriented point cloud. The differentiable PSR layer allows us to efficiently and differentiably bridge the explicit 3D point representation with the 3D mesh via the implicit indicator field, enabling end-to-end optimization of surface reconstruction metrics such as Chamfer distance. This duality between points and meshes hence allows us to represent shapes as oriented point clouds, which are explicit, lightweight and expressive. Compared to neural implicit representations, our Shape-As-Points (SAP) model is more interpretable, lightweight, and accelerates inference time by one order of magnitude. Compared to other explicit representations such as points, patches, and meshes, SAP produces topology-agnostic, watertight manifold surfaces. We demonstrate the effectiveness of SAP on the task of surface reconstruction from unoriented point clouds and learning-based reconstruction.},
  pdf = {http://www.cvlibs.net/publications/Peng2021NEURIPS.pdf},
  supp = {http://www.cvlibs.net/publications/Peng2021NEURIPS_supplementary.pdf},
  code = {https://github.com/autonomousvision/shape_as_points},
  teaser = {sap.gif},
  award = {(Oral)},
  selected = True,
}

@inproceedings{Schwarz2021NEURIPS,
  title = {On the Frequency Bias of Generative Models},
  author = {Schwarz, Katja and Liao, Yiyi and Geiger, Andreas},
  booktitle = NEURIPS,
  year = {2021},
  abstract = {The key objective of Generative Adversarial Networks (GANs) is to generate new data
  with the same statistics as the provided training data. However, multiple recent works show that
  state-of-the-art architectures yet struggle to achieve this goal. In particular, they report an
  elevated amount of high frequencies in the spectral statistics which makes it straightforward to
  distinguish real and generated images. Explanations for this phenomenon are controversial: While
  most works attribute the artifacts to the generator, other works point to the discriminator. We
  take a sober look at those explanations and provide insights on what makes proposed measures
  against high-frequency artifacts effective. To achieve this, we first independently assess the
  architectures of both the generator and discriminator and investigate if they exhibit a frequency
  bias that makes learning the distribution of high-frequency content particularly problematic.
  Based on these experiments, we make the following four observations: 1) Different upsampling
  operations bias the generator towards different spectral properties. 2) Checkerboard artifacts
  introduced by upsampling cannot explain the spectral discrepancies alone as the generator is able
  to compensate for these artifacts. 3) The discriminator does not struggle with detecting high
  frequencies per se but rather struggles with frequencies of low magnitude. 4) The downsampling
  operations in the discriminator can impair the quality of the training signal it provides. In
  light of these findings, we analyze proposed measures against high-frequency artifacts in
  state-of-the-art GAN training but find that none of the existing approaches can fully resolve
  spectral artifacts yet. Our results suggest that there is great potential in improving the
  discriminator and that this could be key to match the distribution of the training data more
  closely.
},
  pdf = {http://www.cvlibs.net/publications/Schwarz2021NEURIPS.pdf},
  supp = {http://www.cvlibs.net/publications/Schwarz2021NEURIPS_supplementary.pdf},
  code = {https://github.com/autonomousvision/frequency_bias},
  teaser = {frequency_bias.gif},
  selected = True,
}

@ARTICLE{Liao2022TPAMI,
  author = {Yiyi Liao and Jun Xie and Andreas Geiger},
  title = {KITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D},
  journal = TPAMI,
  year = {2022},
  abstract = {For the last few decades, several major subfields of artificial intelligence including computer vision, graphics, and robotics have progressed largely independently from each other. Recently, however, the community has realized that progress towards robust intelligent systems such as self-driving cars requires a concerted effort across the different fields. This motivated us to develop KITTI-360, successor of the popular KITTI dataset. KITTI-360 is a suburban driving dataset which comprises richer input modalities, comprehensive semantic instance annotations and accurate localization to facilitate research at the intersection of vision, graphics and robotics. For efficient annotation, we created a tool to label 3D scenes with bounding primitives and developed a model that transfers this information into the 2D image domain, resulting in over  150k images and 1B 3D points with coherent semantic instance annotations across 2D and 3D. Moreover, we established benchmarks and baselines for several tasks relevant to mobile perception, encompassing problems from computer vision, graphics, and robotics on the same dataset, e.g., semantic scene understanding, novel view synthesis and semantic SLAM. KITTI-360 will enable progress at the intersection of these research areas and thus contribute towards solving one of today's grand challenges: the development of fully autonomous self-driving systems.},
  pdf = {http://www.cvlibs.net/publications/Liao2022PAMI.pdf},
  award = {(ESI Highly Cited Paper)},
  code = {http://www.cvlibs.net/datasets/kitti-360/},
  blog = {https://autonomousvision.github.io/kitti-360/},
  teaser = {kitti360.jpg},
}


@inproceedings{Reiser2021ICCV,
  author = {Christian Reiser and Songyou Peng and Yiyi Liao and Andreas Geiger},
  title = {KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs},
  booktitle = ICCV,
  year = {2021},
  abstract = {NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that significant speed-ups are possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by two orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality.},
  pdf = {http://www.cvlibs.net/publications/Reiser2021ICCV.pdf},
  supp = {http://www.cvlibs.net/publications/Reiser2021ICCV_supplementary.pdf},
  code = {https://creiser.github.io/kilonerf/},
  blog = {https://autonomousvision.github.io/kilonerf/},
  teaser = {iccv2021.jpg},
  selected = True,
}

@inproceedings{Riegler2019CVPR,
  title = {Connecting the Dots: Learning Representations for Active Monocular Depth Estimation},
  author = {Riegler, Gernot<sup>*</sup> and Liao, Yiyi<sup>*</sup> and Donné, Simon and Koltun, Vladlen and Geiger, Andreas},
  booktitle = CVPR,
  year = {2019},
  abstract = {We propose a technique for depth estimation with a monocular structured-light camera, \ie, a calibrated stereo set-up with one camera and one laser projector. Instead of formulating the depth estimation via a correspondence search problem, we show that a simple convolutional architecture is sufficient for high-quality disparity estimates in this setting. As accurate ground-truth is hard to obtain, we train our model in a self-supervised fashion with a combination of photometric and geometric losses. Further, we demonstrate that the projected pattern of the structured light sensor can be reliably separated from the ambient information. This can then be used to improve depth boundaries in a weakly supervised fashion by modeling the joint statistics of image and depth edges. The model trained in this fashion compares favorably to the state-of-the-art on challenging synthetic and real-world datasets. In addition, we contribute a novel simulator, which allows to benchmark active depth prediction algorithms in controlled conditions.},
  pdf = {http://www.cvlibs.net/publications/Riegler2019CVPR.pdf},
  supp = {http://www.cvlibs.net/publications/Riegler2019CVPR_supplementary.pdf},
  poster = {http://www.cvlibs.net/publications/Riegler2019CVPR_poster.pdf},
  code = {https://github.com/autonomousvision/connecting_the_dots},
  teaser = {cvpr2019.jpg},
}

@inproceedings{liao2018cvpr,
  title={Deep Marching Cubes: Learning Explicit Surface Representations},
  author={Liao, Yiyi and Donné, Simon and Geiger, Andreas},
  booktitle=CVPR,
  year={2018},
  abstract = {Existing learning based solutions to 3D surface prediction cannot be trained end-to-end as they operate on intermediate representations (eg, TSDF) from which 3D surface meshes must be extracted in a post-processing step (eg, via the marching cubes algorithm). In this paper, we investigate the problem of end-to-end 3D surface prediction. We first demonstrate that the marching cubes algorithm is not differentiable and propose an alternative differentiable formulation which we insert as a final layer into a 3D convolutional neural network. We further propose a set of loss functions which allow for training our model with sparse point supervision. Our experiments demonstrate that the model allows for predicting sub-voxel accurate 3D shapes of arbitrary topology. Additionally, it learns to complete shapes and to separate an object's inside from its outside even in the presence of sparse and incomplete ground truth. We investigate the benefits of our approach on the task of inferring shapes from 3D point clouds. Our model is flexible and can be combined with a variety of shape encoder and shape inference techniques.},
  pdf = {http://www.cvlibs.net/publications/Liao2018CVPR.pdf},
  code = {http://www.cvlibs.net/redirect.php?site=deep_marching_cubes},
  teaser = {cvpr2018.jpg},
}

@inproceedings{Sevilla:GCPR:2018,
  title = {On the Integration of Optical Flow and Action Recognition},
  author = {Sevilla-Lara, Laura and Liao, Yiyi and Guney, Fatma and Jampani, Varun and Geiger, Andreas and Black, Michael},
  booktitle = {German Conference on Pattern Recognition (GCPR)},
  year = {2018},
  abstract = {Most of the top performing action recognition methods use optical flow as a black box input. Here we take a deeper look at the combination of flow and action recognition, and investigate why optical flow is helpful, what makes a flow method good for action recognition, and how we can make it better. In particular, we investigate the impact of different flow algorithms and input transformations to better understand how these affect a state-of-the-art action recognition method. Furthermore, we fine tune two neural-network flow methods end-to-end on the most widely used action recognition dataset (UCF101). Based on these experiments, we make the following five observations: 1) optical flow is useful for action recognition because it is invariant to appearance, 2) optical flow methods are optimized to minimize end-point-error (EPE), but the EPE of current methods is not well correlated with action recognition performance, 3) for the flow methods tested, accuracy at boundaries and at small displacements is most correlated with action recognition performance, 4) training optical flow to minimize classification error instead of minimizing EPE improves recognition performance, and 5) optical flow learned for the task of action recognition differs from traditional optical flow especially inside the human body and at the boundary of the body. These observations may encourage optical flow researchers to look beyond EPE as a goal and guide action recognition researchers to seek better motion cues, leading to a tighter integration of the optical flow and action recognition communities.},
  pdf = {http://www.cvlibs.net/publications/Sevilla-Lara2018GCPR.pdf},
  teaser = {gcpr2018.jpg},
}




@inproceedings{Schwarz2020NEURIPS,
  title = {GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis},
  author = {Schwarz, Katja<sup>*</sup> and Liao, Yiyi<sup>*</sup> and Niemeyer, Michael and Geiger, Andreas},
  booktitle = NEURIPS,
  year = {2020},
  abstract = {While 2D generative adversarial networks have enabled high-resolution image synthesis,
  they largely lack an understanding of the 3D world and the image formation process. Thus, they do
  not provide precise control over camera viewpoint or object pose. To address this problem, several
  recent approaches leverage intermediate voxel-based representations in combination with differentiable
  rendering. However, existing methods either produce low image resolution or fall short in disentangling
  camera and scene properties, e.g., the object identity may vary with the viewpoint. In this paper,
  we propose a generative model for radiance fields which have recently proven successful for novel view
  synthesis of a single scene. In contrast to voxel-based representations, radiance fields are not
  confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene
  properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a
  multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training
  our model from unposed 2D images alone. We systematically analyze our approach on several challenging
  synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful
  representation for generative image synthesis, leading to 3D consistent models that render with high
  fidelity.
},
  pdf = {http://www.cvlibs.net/publications/Schwarz2020NEURIPS.pdf},
  supp = {http://www.cvlibs.net/publications/Schwarz2020NEURIPS_supplementary.pdf},
  code = {https://github.com/autonomousvision/graf},
  blog = {https://autonomousvision.github.io/graf/},
  poster = {http://www.cvlibs.net/publications/Schwarz2020NEURIPS_poster.pdf},
  video = {https://www.youtube.com/watch?v=akQf7WaCOHo},
  teaser = {graf.gif},
  selected = True,
}

@inproceedings{Liao2020CVPR,
  title = {Towards Unsupervised Learning of Generative Models for 3D Controllable Image Synthesis},
  author = {Liao, Yiyi<sup>*</sup> and Schwarz, Katja<sup>*</sup> and Mescheder, Lars and Geiger, Andreas},
  booktitle = CVPR,
  year = {2020},
  abstract = {In recent years, Generative Adversarial Networks have achieved impressive results
   in photorealistic image synthesis. This progress nurtures hopes that one day the classical
   rendering pipeline can be replaced by efficient models that are learned directly from images.
   However, current image synthesis models operate in the 2D domain where disentangling 3D properties
   such as camera viewpoint or object pose is challenging. Furthermore, they lack an interpretable
   and controllable representation. Our key hypothesis is that the image generation process should
   be modeled in 3D space as the physical world surrounding us is intrinsically three-dimensional.
   We define the new task of 3D controllable image synthesis and propose an approach for solving it
   by reasoning both in 3D space and in the 2D image domain. We demonstrate that our model is able to
   disentangle latent 3D factors of simple multi-object scenes in an unsupervised fashion from raw
   images. Compared to pure 2D baselines, it allows for synthesizing scenes that are consistent wrt.
   changes in viewpoint or object pose. We further evaluate various 3D representations in terms of
   their usefulness for this challenging task.
   },
  pdf = {http://www.cvlibs.net/publications/Liao2020CVPR.pdf},
  supp = {http://www.cvlibs.net/publications/Liao2020CVPR_supplementary.pdf},
  code = {https://github.com/autonomousvision/controllable_image_synthesis},
  blog = {https://autonomousvision.github.io/controllable-gan/},
  poster = {http://www.cvlibs.net/publications/Liao2020CVPR_poster.pdf},
  video = {https://www.youtube.com/watch?v=ygQCgGC0Lm8},
  teaser = {controllableGAN.gif},
  selected = True,
}

@inproceedings{Tosi2021CVPR,
  title={SMD-Nets: Stereo Mixture Density Networks},
  author={Tosi, Fabio and Liao, Yiyi and Schmitt, Carolin and Geiger, Andreas},
  booktitle = CVPR,
  year = {2021},
  abstract = {Despite stereo matching accuracy has greatly improved by deep learning in the last few years, recovering sharp boundaries and high-resolution outputs efficiently remains challenging. In this paper, we propose Stereo Mixture Density Networks (SMD-Nets), a simple yet effective learning framework compatible with a wide class of 2D and 3D architectures which ameliorates both issues. Specifically, we exploit bimodal mixture densities as output representation and show that this allows for sharp and precise disparity estimates near discontinuities while explicitly modeling the aleatoric uncertainty inherent in the observations. Moreover, we formulate disparity estimation as a continuous problem in the image domain, allowing our model to query disparities at arbitrary spatial precision. We carry out comprehensive experiments on a new high-resolution and highly realistic synthetic stereo dataset, consisting of stereo pairs at 8Mpx resolution, as well as on real-world stereo datasets. Our experiments demonstrate increased depth accuracy near object boundaries and prediction of ultra high-resolution disparity maps on standard GPUs. We demonstrate the flexibility of our technique by improving the performance of a variety of stereo backbones.},
  pdf = {http://www.cvlibs.net/publications/Tosi2021CVPR.pdf},
  supp = {http://www.cvlibs.net/publications/Tosi2021CVPR_supplementary.pdf},
  code = {https://github.com/fabiotosi92/SMD-Nets},
  blog = {https://autonomousvision.github.io/smdnets/},
  teaser = {cvpr2021.jpg},
  selected = True,
}

@article{Liu2021TIP,
  title={Learning Steering Kernels for Guided Depth Completion},
  author={Liu, Lina and Liao, Yiyi<sup>&#x0266F;</sup>, and Wang, Yue and Geiger, Andreas and Liu, Yong<sup>&#x0266F;</sup>},
  journal=TIP,
  volume={30},
  pages={2850--2861},
  year={2021},
  abstract={This paper addresses the guided depth completion
        task in which the goal is to predict a dense depth map given a
        guidance RGB image and sparse depth measurements. Recent
        advances on this problem nurture hopes that one day we can
        acquire accurate and dense depth at a very low cost. A major
        challenge of guided depth completion is to effectively make use
        of extremely sparse measurements, e.g., measurements covering
        less than 1\% of the image pixels. In this paper, we propose
        a fully differentiable model that avoids convolving on sparse
        tensors by jointly learning depth interpolation and refinement.
        More specifically, we propose a differentiable kernel regression
        layer that interpolates the sparse depth measurements via learned
        kernels. We further refine the interpolated depth map using
        a residual depth refinement layer which leads to improved
        performance compared to learning absolute depth prediction
        using a vanilla network. We provide experimental evidence that
        our differentiable kernel regression layer not only enables end-to-end training from very sparse measurements using standard
        convolutional network architectures, but also leads to better
        depth interpolation results compared to existing heuristically
        motivated methods. We demonstrate that our method outperforms many state-of-the-art guided depth completion techniques
        on both NYUv2 and KITTI. We further show the generalization
        ability of our method with respect to the density and spatial
        statistics of the sparse depth measurements.},
  pdf = {http://www.cvlibs.net/publications/Liu2021TIP.pdf},
  code = {https://github.com/LINA-lln/Steering-KernelNet},
  teaser = {tip2021.jpg},
  selected = True,
}

@article{liao2017graph,
  title={Graph Regularized Auto-encoders for Image Representation},
  author={Liao, Yiyi and Wang, Yue and Liu, Yong},
  journal=TIP,
  volume={26},
  number={6},
  pages={2839--2852},
  year={2017},
  publisher={IEEE},
  abstract={Image representation has been intensively explored in the domain of computer vision for its significant influence on the relative tasks such as image clustering and classification. It is valuable to learn a low-dimensional representation of an image which preserves its inherent information from the original image space. At the perspective of manifold learning, this is implemented with the local invariant idea to capture the intrinsic low-dimensional manifold embedded in the high-dimensional input space. Inspired by the recent successes of deep architectures, we propose a local invariant deep nonlinear mapping algorithm, called graph regularized auto-encoder (GAE). With the graph regularization, the proposed method preserves the local connectivity from the original image space to the representation space, while the stacked auto-encoders provide explicit encoding model for fast inference and powerful expressive capacity for complex modeling. Theoretical analysis shows that the graph regularizer penalizes the weighted Frobenius norm of the Jacobian matrix of the encoder mapping, where the weight matrix captures the local property in the input space. Furthermore, the underlying effects on the hidden representation space are revealed, providing insightful explanation to the advantage of the proposed method. Finally, the experimental results on both clustering and classification tasks demonstrate the effectiveness of our GAE as well as the correctness of the proposed theoretical analysis, and it also suggests that GAE is a superior solution to the current deep representation learning techniques comparing with variant auto-encoders and existing local invariant methods.
},
  pdf={https://ieeexplore.ieee.org/document/7556994},
  teaser = {tip2017.jpg}
}

@article{liao2017place,
  title={Place Classification with a Graph Regularized Deep Neural Network},
  author={Liao, Yiyi and Kodagoda, Sarath and Wang, Yue and Shi, Lei and Liu, Yong},
  journal={IEEE Trans. on Cognitive and Developmental Systems (TCDS)},
  volume={9},
  number={4},
  pages={304--315},
  year={2017},
  abstract={Place classification is a fundamental ability that a robot should possess to carry out effective human-robot interactions. In recent years, there is a high exploitation of artificial intelligence algorithms in robotics applications. Inspired by the recent successes of deep learning methods, we propose an end-to-end learning approach for the place classification problem. With deep architectures, this methodology automatically discovers features and contributes in general to higher classification accuracies. The pipeline of our approach is composed of three parts. First, we construct multiple layers of laser range data to represent the environment information in different levels of granularity. Second, each layer of data are fed into a deep neural network for classification, where a graph regularization is imposed to the deep architecture for keeping local consistency between adjacent samples. Finally, the predicted labels obtained from all layers are fused based on confidence trees to maximize the overall confidence. Experimental results validate the effectiveness of our end-to-end place classification framework in which both the multilayer structure and the graph regularization promote the classification performance. Furthermore, results show that the features automatically learned from the raw input range data can achieve competitive results to the features constructed based on statistical and geometrical information.
},
  pdf = {https://ieeexplore.ieee.org/document/7501830},
  teaser = {tcds2017.jpg},
  selected = False,
}

@inproceedings{liao2017parse,
  title={Parse Geometry from a Line: Monocular Depth Estimation with Partial Laser Observation},
  author={Liao, Yiyi and Huang, Lichao and Wang, Yue and Kodagoda, Sarath and Yu, Yinan and Liu, Yong},
  booktitle=ICRA,
  year={2017},
  abstract={Many standard robotic platforms are equipped with at least a fixed 2D laser range finder and a monocular camera. Although those platforms do not have sensors for 3D depth sensing capability, knowledge of depth is an essential part in many robotics activities. Therefore, recently, there is an increasing interest in depth estimation using monocular images. As this task is inherently ambiguous, the data-driven estimated depth might be unreliable in robotics applications. In this paper, we have attempted to improve the precision of monocular depth estimation by introducing 2D planar observation from the remaining laser range finder without extra cost. Specifically, we construct a dense reference map from the sparse laser range data, redefining the depth estimation task as estimating the distance between the real and the reference depth. To solve the problem, we construct a novel residual of residual neural network, and tightly combine the classification and regression losses for continuous depth estimation. Experimental results suggest that our method achieves considerable promotion compared to the state-of-the-art methods on both NYUD2 and KITTI, validating the effectiveness of our method on leveraging the additional sensory information. We further demonstrate the potential usage of our method in obstacle avoidance where our methodology provides comprehensive depth information compared to the solution using monocular camera or 2D laser range finder alone.},
  pdf={https://arxiv.org/pdf/1611.02174.pdf},
  teaser={icra2017.jpg},
  selected={true},
}

@inproceedings{liao2016understand,
  title={Understand Scene Categories by Objects: A Semantic Regularized Scene Classifier Using Convolutional Neural Networks},
  author={Liao, Yiyi and Kodagoda, Sarath and Wang, Yue and Shi, Lei and Liu, Yong},
  booktitle=ICRA,
  pages={2318--2325},
  year={2016},
  abstract={Scene classification is a fundamental perception task for environmental understanding in today's robotics. In this paper, we have attempted to exploit the use of popular machine learning technique of deep learning to enhance scene understanding, particularly in robotics applications. As scene images have larger diversity than the iconic object images, it is more challenging for deep learning methods to automatically learn features from scene images with less samples. Inspired by human scene understanding based on object knowledge, we address the problem of scene classification by encouraging deep neural networks to incorporate object-level information. This is implemented with a regularization of semantic segmentation. With only 5 thousand training images, as opposed to 2.5 million images, we show the proposed deep architecture achieves superior scene classification results to the state-of-the-art on a publicly available SUN RGB-D dataset. In addition, performance of semantic segmentation, the regularizer, also reaches a new record with refinement derived from predicted scene labels. Finally, we apply our SUN RGB-D dataset trained model to a mobile robot captured images to classify scenes in our university demonstrating the generalization ability of the proposed algorithm.
},
  pdf={https://arxiv.org/pdf/1509.06470.pdf},
  teaser={icra2016.jpg},
}

